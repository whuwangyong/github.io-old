[{"categories":["小工具"],"content":"有时候电脑右下角、正中间弹出的广告，没有任何logo和落款，不知道是哪个软件搞的。这就好比挨了一拳，没看见出拳的人。 微软的一个大佬开发的 Process Explorer 功能强大，能解决这个问题。将工具栏上的瞄准镜拖动到广告弹窗上面即可。 ","date":"2022-07-23","objectID":"/2022-07-23-use-process-explorer-to-detect-ad-process/:0:0","tags":["Process Explorer","弹窗广告"],"title":"Process Explorer检测弹窗广告的利器","uri":"/2022-07-23-use-process-explorer-to-detect-ad-process/"},{"categories":["kafka"],"content":"问题 有个需求，需要频繁seek到指定partition的指定offset，然后poll，且只poll一次，目的是为了快速将指定offset的消息拉取出来。 通常的poll写法是，将poll逻辑放在死循环里，第一次拉不到，第二次继续。如果offset上有消息，就一定能消费到： consumer.subscribe(\"topics\"); while(true){ records = consumer.poll(Duration.ofSeconds(1)); // do something with records } 但我使用的是consumer.assign()方法，而不是subscribe()。因为要灵活指定分区，用subscribe的话，触发rebalance很麻烦。代码如下： public ConsumerRecord\u003cString, String\u003e seekAndPoll(String topic, int partition, long offset) { TopicPartition tp = new TopicPartition(topic, partition); consumer.assign(Collections.singleton(tp)); System.out.println(\"assignment:\" + consumer.assignment()); // 这里是有分配到分区的 consumer.seek(tp, offset); ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100)) if(records.isEmpty()){ // 大概率拉取不到消息，进入此分支 return null; } else { return records.iterator().next(); } } 由于我只poll一次，这就要求必须一次拉到消息。从现象上看，感觉是在seek之后，kafka有些metadata更新之类的操作未执行完毕，此时poll就拉不到消息。 我在StackOverflow上也搜到了这个问题（java - Kafka Cluster sometimes returns no records during seek and poll - Stack Overflow），但是没有答案。在解决了这个问题后，我添加了一个答案。 ","date":"2022-05-31","objectID":"/2022-02-17-kafka-seek-poll/:1:0","tags":["kafka","kafka consumer"],"title":"kafka consumer seek之后立即poll可能拉不到消息","uri":"/2022-02-17-kafka-seek-poll/"},{"categories":["kafka"],"content":"分析 ","date":"2022-05-31","objectID":"/2022-02-17-kafka-seek-poll/:2:0","tags":["kafka","kafka consumer"],"title":"kafka consumer seek之后立即poll可能拉不到消息","uri":"/2022-02-17-kafka-seek-poll/"},{"categories":["kafka"],"content":"猜测1 新旧poll方法的区别 在测试时，发现有时使用旧版本的poll(long timeout)方法有效，使用新版本的poll(Duration timeout)方法无效。会不会跟这个有关？（调式发现无关，不感兴趣的可跳过这一节） 两个poll方法签名如下： @Deprecated public ConsumerRecords\u003cK, V\u003e poll(final long timeoutMs) { return poll(time.timer(timeoutMs), false); } public ConsumerRecords\u003cK, V\u003e poll(final Duration timeout) { return poll(time.timer(timeout), true); } 二者都调用了下面的这个poll方法，关键在于第二个参数includeMetadataInTimeout，新版为false，老版为true。 private ConsumerRecords\u003cK, V\u003e poll(final Timer timer, final boolean includeMetadataInTimeout) { // 略 if (includeMetadataInTimeout) { // try to update assignment metadata BUT do not need to block on the timer for join group updateAssignmentMetadataIfNeeded(timer, false); } else { while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), true)) { log.warn(\"Still waiting for metadata\"); } } // 略 } 这个Boolean值最终传递给了coordinator.poll()的waitForJoinGroup。因此，关键就在于coordinator在poll的时候是否等待消费者成功加入消费组。 boolean updateAssignmentMetadataIfNeeded(final Timer timer, final boolean waitForJoinGroup) { if (coordinator != null \u0026\u0026 !coordinator.poll(timer, waitForJoinGroup)) { return false; } return updateFetchPositions(timer); } 但调试发现，在使用assign手动指定消费分区时，coordinator 为 null。这很好理解，只有subscribe模式才存在重平衡等情况，需要coordinator进行协调。 所以能否拉取到消息，与poll是新版还是旧版无关。 延伸阅读，关于poll方法改版的KIP： KIP-266: Fix consumer indefinite blocking behavior 关键内容摘抄如下： Consumer#poll The pre-existing variant poll(long timeout) would block indefinitely for metadata updates if they were needed, then it would issue a fetch and poll for timeout ms for new records. The initial indefinite metadata block caused applications to become stuck when the brokers became unavailable. The existence of the timeout parameter made the indefinite block especially unintuitive. We will add a new method poll(Duration timeout) with the semantics: iff a metadata update is needed: send (asynchronous) metadata requests poll for metadata responses (counts against timeout) if no response within timeout, return an empty collection immediately if there is fetch data available, return it immediately if there is no fetch request in flight, send fetch requests poll for fetch responses (counts against timeout) if no response within timeout, return an empty collection (leaving async fetch request for the next poll) if we get a response, return the response We will deprecate the original method, poll(long timeout), and we will not change its semantics, so it remains: iff a metadata update is needed: send (asynchronous) metadata requests poll for metadata responses indefinitely until we get it if there is fetch data available, return it immediately if there is no fetch request in flight, send fetch requests poll for fetch responses (counts against timeout) if no response within timeout, return an empty collection (leaving async fetch request for the next poll) if we get a response, return the response One notable usage is prohibited by the new poll: previously, you could call poll(0) to block for metadata updates, for example to initialize the client, supposedly without fetching records. Note, though, that this behavior is not according to any contract, and there is no guarantee that poll(0) won’t return records the first time it’s called. Therefore, it has always been unsafe to ignore the response. 简言之，poll(long timeout) 是无限期阻塞的，会等待订阅的元数据信息更新完成（这个等待时间不包含在timeout之内），确保能拉到消息。而poll(Duration timeout)不会一直阻塞，经过最多timeout后就会返回，不管拉没拉到消息。 ","date":"2022-05-31","objectID":"/2022-02-17-kafka-seek-poll/:2:1","tags":["kafka","kafka consumer"],"title":"kafka consumer seek之后立即poll可能拉不到消息","uri":"/2022-02-17-kafka-seek-poll/"},{"categories":["kafka"],"content":"猜测2 timeout决定了一切 在调式过程中，我发现不管用用新版的poll，还是老版本的poll，当timeout太小时，如10ms，第一次poll多半拉不到消息；timeout足够大时，比如2000ms，每次都拉到消息了。于是我得出结论：将timeout设置足够大即可。不足的是，如果传入的offset参数越界，该位置本来就没有消息，poll方法也会等待timeout才返回（这里或许是kafka的一个待优化的点？），浪费时间，于是我决定加一个检查，当传入的offset超过partition的起始偏移量时，快速返回。代码如下： public ConsumerRecord\u003cString, String\u003e seekAndPoll(String topic, int partition, long offset) { TopicPartition tp = new TopicPartition(topic, partition); consumer.assign(Collections.singleton(tp)); System.out.println(\"assignment:\" + consumer.assignment()); // 这里是有分配到分区的 // endOffset: the offset of the last successfully replicated message plus one // if there has 5 messages, valid offsets are [0,1,2,3,4], endOffset is 4+1=5 Long endOffset = consumer.endOffsets(Collections.singleton(tp)).get(tp); Long beginOffset = consumer.beginningOffsets(Collections.singleton(tp)).get(tp); if (offset \u003c beginOffset || offset \u003e= endOffset) { System.out.println(\"offset is illegal\"); return null; } else { consumer.seek(tp, offset); ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(2000)) if(records.isEmpty()){ return null; } else { return records.iterator().next(); } } } ","date":"2022-05-31","objectID":"/2022-02-17-kafka-seek-poll/:2:2","tags":["kafka","kafka consumer"],"title":"kafka consumer seek之后立即poll可能拉不到消息","uri":"/2022-02-17-kafka-seek-poll/"},{"categories":["kafka"],"content":"真相？ consumer.endOffsets() 我做了一个测试，在2个topic的4个partition上反复执行猜测2的代码，循环10000次，并更改timeout的大小，期望得出timeout值的大小与seekAndPoll失败之间量化关系。结果发现，即使timeout只有10ms，poll也有非常高的成功率；timeout=50ms时，poll成功率就能达到100%。而之前要timeout=1000ms ~ 2000ms才能有这么高的成功率。我反复检查，最终发现是这两行代码造成的： Long beginOffset = consumer.beginningOffsets(Collections.singleton(tp)).get(tp); Long endOffset = consumer.endOffsets(Collections.singleton(tp)).get(tp); 点进endOffsets方法浏览了一下，底下大概做了fetchOffsetsByTimes、awaitMetadataUpdate等事情。但是consumer.endOffsets()方法也不是万无一失的，当timeout=5ms时，poll成功率为97%。 测试代码见文末。 ","date":"2022-05-31","objectID":"/2022-02-17-kafka-seek-poll/:2:3","tags":["kafka","kafka consumer"],"title":"kafka consumer seek之后立即poll可能拉不到消息","uri":"/2022-02-17-kafka-seek-poll/"},{"categories":["kafka"],"content":"结论 就按照上述猜测2的代码写，timeout设置为2000ms或者更大。只要seek传入的offset通过了检查，那么该offset上一定有消息，poll时就会立即返回。因此这个timeout即使设置很大也无影响。 consumer.beginOffsets()和consumer.endOffsets()一方面起到了检查offset的作用，另一方面起到了降低timeout的作用（虽然这并不是目的）。 注意，在beginOffset和endOffset确定的情况下进行多次seek，不需要每次都调API去查询，而是应该缓存起来多次使用。在这种情况下，能一次poll到消息就靠timeout了，因此，不要把timeout设置得太小，至少100ms。 ","date":"2022-05-31","objectID":"/2022-02-17-kafka-seek-poll/:3:0","tags":["kafka","kafka consumer"],"title":"kafka consumer seek之后立即poll可能拉不到消息","uri":"/2022-02-17-kafka-seek-poll/"},{"categories":["kafka"],"content":"测试代码 kafka-test/SeekTest.java at main · whuwangyong/kafka-test (github.com) ","date":"2022-05-31","objectID":"/2022-02-17-kafka-seek-poll/:4:0","tags":["kafka","kafka consumer"],"title":"kafka consumer seek之后立即poll可能拉不到消息","uri":"/2022-02-17-kafka-seek-poll/"},{"categories":["kafka"],"content":"Kafka的事务是什么 生产者往多个topic里面写消息，要么同时成功，要么同时失败。 ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:1","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"为什么需要事务 消息系统有3种语义： 最多一次 最少一次 精确一次。Exactly Only Once 为了实现精确一次的语义，Kafka必须引入事务。如下图： 本应用从上游topic消费消息，处理后发到下游topic，同时将处理进度发送到__consumer_offsetstopic里面进行保存，对这两个topic的写，是一个原子操作（atomic）。 如果没有事务，发生以下情况时，消息可能会重复或者丢失： A broker can fail The producer-to-broker RPC can fail The client can fail ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:2","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"Zombie instances 在分布式系统中，主节点由于网络闪断等原因，被一致性协议踢出，过了一会儿又恢复过来，并希望与现任主节点做同样的事情。前任主节点就是zombie instances。 ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:3","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"Zombie fencing We solve the problem of zombie instances by requiring that each transactional producer be assigned a unique identifier called the transactional.id . This is used to identify the same producer instance across process restarts. transactional.id 能够跨越进程重启。也就是说，当主节点宕机后，备节点使用主节点的transactional.id 继续干活，是可以的。也就说，事务id “is consistent across producer sessions”。 ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:4","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"代码 //Read and validate deposits validated Deposits = validate(consumer.poll(0)) //Send validated deposits \u0026 commit offsets atomically producer.beginTransaction() producer.send(validatedDeposits) producer.sendOffsetsToTransaction(offsets(consumer)) producer.endTransaction() ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:5","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"How Transactions Work ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:6","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"The Transaction Coordinator and Transaction Log The transaction coordinator is a module running inside every Kafka broker. The transaction log is an internal kafka topic. Each coordinator owns some subset of the partitions in the transaction log, ie. the partitions for which its broker is the leader. Every transactional.id is mapped to a specific partition of the transaction log through a simple hashing function. This means that exactly one coordinator owns a given transactional.id. 每个broker里面都有一个事务协调者。transaction log是一个内部topic。 ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:7","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"Data flow 4个阶段 生产者与协调者的交互 生产者使用initTransactions API向协调者注册一个事务id。此时，协调者会close所有具有相同事务id且处于pending状态的事务，并把其epoch 纳入 fence out zombies 生产者在一个事务里第一次对一个分区发消息时，该分区会在协调者注册 应用调用commit或abort方法时，客户端会发一个请求到协调者，协调者开始做二阶段提交 协调者与transaction log的交互 协调者是唯一一个读写transaction log的组件。 生产者写数据到目标分区 协调者与目标分区的交互 生产者commit或abort时，协调者开始执行2PC： 在内存更新事务状态为“prepare_commit”，也更新transaction log里的状态 将 commit markers 写到本事务涉及到的topic-partitions 更新事务状态为“complete” ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:8","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"How to pick a transactional.id The transactional.id plays a major role in fencing out zombies. But maintaining an identifier that is consistent across producer sessions and also fences out zombies properly is a bit tricky. The key to fencing out zombies properly is to ensure that the input topics and partitions in the read-process-write cycle is always the same for a given transactional.id. If this isn’t true, then it is possible for some messages to leak through the fencing provided by transactions. For instance, in a distributed stream processing application, suppose topic-partition tp0 was originally processed by transactional.id *T0. *If, at some point later, it could be mapped to another producer with transactional.id *T1, *there would be no fencing between T0 and *T1. *So it is possible for messages from tp0 to be reprocessed, violating the exactly-once processing guarantee. Practically, one would either have to store the mapping between input partitions and transactional.ids in an external store, or have some static encoding of it. Kafka Streams opts for the latter approach to solve this problem. ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:9","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"How transactions perform, and how to tune them Performance of the transactional producer 事务的性能开销 Transactions cause only moderate write amplification（有限的写放大）. The additional writes are due to: additional RPCs to register the partitions with the coordinator transaction marker has to be written to each partition write state changes to the transaction log 事务提交的批次越小、间隔越小，性能损耗越大。对于1KB消息，每100ms提交一次，吞吐下降3%。但是，批次（间隔）越大，消息的实时性越差。 事务对消费者来说几乎没性能损失。 ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:10","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"幂等producer与事务producer 幂等producer，其id是没持久化的。重启后会变。 事务producer，其id是事务id。这个id在producer重启后也不会变。 虽然producer重启后事务id不会变，但是，一个新的producer session创建时，会再次执行initTransactions()，epoch会增加。假设旧的producer并没彻底宕机，当它恢复过来时，尝试使用旧的epoch提交事务，就会报错。 ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:11","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"事务型消费者，可以消费未启用事务的普通消息吗 可以。 官方文档对isolation.level的描述： If set to read_committed , consumer.poll() will only return transactional messages which have been committed. If set to read_uncommitted(the default), consumer.poll() will return all messages, even transactional messages which have been aborted. Non-transactional messages will be returned unconditionally in either mode. 最后一句：不管在哪种模式下，非事务的消息都是无条件返回。 ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:12","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"如何实现跨数据源的事务：kafka+DB混合事务 对于如下的一个逻辑： 本应用消费上游topic的消息，然后将结果1发送到下游topic，同时将结果2写入db，另外还要提交消费进度到__consumer_offsetstopic。如何保证这3个写操作是一个事务？kafka的事务并不支持跨数据源。官方文档如下： When writing to an external system, the limitation is in the need to coordinate the consumer’s position with what is actually stored as output. The classic way of achieving this would be to introduce a two-phase commit between the storage of the consumer position and the storage of the consumers output. But this can be handled more simply and generally by letting the consumer store its offset in the same place as its output. —— Apache Kafka Documentation - 4.6 Message Delivery Semantics The main restriction with Transactions is they only work in situations where both the input comes from Kafka and the output is written to a Kafka topic. If you are calling an external service (e.g., via HTTP), updating a database, writing to stdout, or anything other than writing to and from the Kafka broker, transactional guarantees won’t apply and calls can be duplicated. This is exactly how a transactional database works: the transaction works only within the confines of the database, but because Kafka is often used to link systems it can be a cause of confusion. Put another way, Kafka’s transactions are not inter-system transactions such as those provided by technologies that implement XA. —— What Can’t Transactions Do? 官方建议，“letting the consumer store its offset in the same place as its output”，将输出保存到同一数据源：要么全部写数据库（消费进度也存在数据库里），此时可以用数据库的事务；要么全部写kafka。 根据这种思路，提供设计如下： 结果2也写到kafka里面，这样写结果1+结果2+消费进度是一个kafka事务，可以做到精确一次消费。然后单独写一个持久化线程，将数据从kafka里面消费，写到db。注意写db的时候，需要将消费进度一起写入db，利用数据库事务来确保精确一次持久化。 ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:13","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"Reference Exactly Once Processing in Kafka with Java | Baeldung Kafka: The Definitive Guide(2nd Edition) Apache Kafka Documentation Transactions in Apache Kafka | Confluent Building Systems Using Transactions in Apache Kafka® (confluent.io) Exactly-once Semantics is Possible: Here’s How Apache Kafka Does it (confluent.io) ","date":"2022-05-30","objectID":"/2022-05-30-kafka-tx/:0:14","tags":["kafka","kafka事务"],"title":"Kafka事务不完全指南","uri":"/2022-05-30-kafka-tx/"},{"categories":["kafka"],"content":"why kafka offset not sequential 未使用事务时，至少一次语义，消息重发时，会占用offset 使用事务时，每次事务的commit/abort，都会往topic（每个分区？）写一个标志，这个标志会占用offset 官方并未提及offset是连续的 ","date":"2022-05-20","objectID":"/2022-05-20-why-kafka-offset-not-sequential/:1:0","tags":["kafka","kafka offset"],"title":"kafka offset为什么不连续","uri":"/2022-05-20-why-kafka-offset-not-sequential/"},{"categories":["kafka"],"content":"Reference [Solved] Kafka Streams does not increment offset by 1 when producing to topic - Local Coder ","date":"2022-05-20","objectID":"/2022-05-20-why-kafka-offset-not-sequential/:2:0","tags":["kafka","kafka offset"],"title":"kafka offset为什么不连续","uri":"/2022-05-20-why-kafka-offset-not-sequential/"},{"categories":["静态网站博客"],"content":"建站之后，为了文章能够被搜索引擎收录，我们需要将站点提交到Google、百度、Bing等搜索网站，通过验证之后，搜索引擎才会去我们的网站爬数据。 为了方便爬虫爬取我们站点里的文章，我们可以将站点地图（sitemap.xml）提交到搜索网站。提交之后，爬虫在光临我们的网站时，会根据sitemap.xml的指引，抓取所有的URL。 但是，爬虫光临我们站点的周期太长（至少要几天吧），如果想发布文章之后尽快被搜索引擎收录，我们可以主动提交URL到搜索网站。Google只能在网页上操作，百度和Bing都提供了API。下面介绍如何使用API提交URL。 ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:0:0","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"生成url列表 在提交url之前，需先准备好url列表，形如： http://www.your-site.com/1.html http://www.your-site.com/2.html 有两种方式生成url列表。 ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:1:0","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"从sitemap.xml获取 多数静态网站构建工具（Hugo、Jekyll）都会生成sitemap.xml，形如： \u003curlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\" xmlns:xhtml=\"http://www.w3.org/1999/xhtml\"\u003e \u003curl\u003e \u003cloc\u003ehttps://whuwangyong.github.io/2022-03-29-github-submodule/\u003c/loc\u003e \u003clastmod\u003e2022-04-27T23:14:33+08:00\u003c/lastmod\u003e \u003cchangefreq\u003eweekly\u003c/changefreq\u003e \u003cpriority\u003e1\u003c/priority\u003e \u003c/url\u003e \u003curl\u003e \u003cloc\u003ehttps://whuwangyong.github.io/2022-04-27-linux-cpu-benchmark/\u003c/loc\u003e \u003clastmod\u003e2022-04-27T23:14:33+08:00\u003c/lastmod\u003e \u003cchangefreq\u003eweekly\u003c/changefreq\u003e \u003cpriority\u003e1\u003c/priority\u003e \u003c/url\u003e \u003c/urlset\u003e 用以下Linux命令可以将sitemap.xml中的url提取出来： grep \"\u003cloc\u003e\" sitemap.xml | grep -v 'html' | awk -F '[\u003c \u003e]' '{print $3}' \u003e urls.txt 这样做的弊端是每次提交的url列表是全量的。而百度和Bing都限制了每天提交的数量，百度 3000条/天，Bing 100条/天。随着文章增多，可能会超限。因此，更好的做法是，每次只提交本次更新的文章的url，通过git log可以实现。 ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:1:1","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"从git log获取 # 获取最近一次的commit_id \u003e git rev-parse --short HEAD 2bfe64f # 显示本次提交修改的文件 \u003e git diff-tree --no-commit-id --name-only -r 2bfe64f 2022-03-29-github-submodule/assets/image-20220404003800-3hsabzt.png 2022-03-29-github-submodule/index.html 2022-03-29-github-submodule/index.md 2022-04-27-linux-cpu-benchmark/index.html 2022-04-27-linux-cpu-benchmark/index.md index.json sitemap.xml # 显示本次提交修改的文件 （另一种方法） \u003e git show --pretty=\"\" --name-only 2bfe64f 2022-03-29-github-submodule/assets/image-20220404003800-3hsabzt.png 2022-03-29-github-submodule/index.html 2022-03-29-github-submodule/index.md 2022-04-27-linux-cpu-benchmark/index.html 2022-04-27-linux-cpu-benchmark/index.md index.json sitemap.xml 然后从修改文件列表中，删选出.html结尾的路径，与站点根目录拼起来，得到完整url。后文有提供python代码来生成url列表。 准备好了url列表，下面开始使用API提交。 ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:1:2","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"百度收录API提交 登陆百度搜索资源平台-站点管理，选择资源提交-普通收录-API提交： 旁边的sitemap用于提交sitemap.xml文件，这个很简单，不多说： 百度给出了4种API提交的方法： ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:2:0","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"curl推送 将要提交的链接按照每行一条的格式写入一个文本文件中，命名此文件为urls.txt，然后执行： curl -H 'Content-Type:text/plain' --data-binary @urls.txt \"http://data.zz.baidu.com/urls?site=https://whuwangyong.github.io\u0026token=xxxxx\" ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:2:1","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"post推送 也很简单，具体可以参考文末代码。 POST /urls?site=https://whuwangyong.github.io\u0026token=your-token HTTP/1.1 User-Agent: curl/7.12.1 Host: data.zz.baidu.com Content-Type: text/plain data = [ \"http://www.example.com/1.html\", \"http://www.example.com/2.html\" ] ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:2:2","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"php推送、ruby推送 未尝试。 ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:2:3","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"Bing Webmaster API 提交 登录Bing Webmaster Tools，点击右上角设置-API访问，获取api key。 然后通过URL提交进入提交API页面： Bing提供了两种提交格式，json和xml。我使用的是json。接口约定如下： JSON request sample: POST /webmaster/api.svc/json/SubmitUrlbatch?apikey=sampleapikeyEDECC1EA4AE341CC8B6 HTTP/1.1 Content-Type: application/json; charset=utf-8 Host: ssl.bing.com { \"siteUrl\":\"http://yoursite.com\", \"urlList\":[ \"http://yoursite.com/url1\", \"http://yoursite.com/url2\", \"http://yoursite.com/url3\" ] } JSON response sample: HTTP/1.1 200 OK Content-Length: 10 Content-Type: application/json; charset=utf-8 { \"d\":null } ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:3:0","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["静态网站博客"],"content":"代码 def commit_urls(): print(\"将最新的url提交到百度和bing\") os.system(\"git checkout gh-pages\") urls = [] # 生成url列表 ret = subprocess.run( \"git rev-parse --short HEAD\", stdout=subprocess.PIPE, stderr=subprocess.PIPE ) if ret.returncode == 0: commit_id = str(ret.stdout, \"utf_8\").strip() ret = subprocess.run( \"git show --pretty=\" \" --name-only \" + commit_id, stdout=subprocess.PIPE, stderr=subprocess.PIPE, ) if ret.returncode == 0: changes = str(ret.stdout, \"utf-8\").split(\"\\n\") for change in changes: if change.endswith(\".html\"): # change[:-10] 是为了去掉末尾的index.html urls.append(\"https://whuwangyong.github.io/{}\".format(change[:-10])) else: print(\"subprocess run error:{}\".format(ret.stderr)) else: print(\"subprocess run error:{}\".format(ret.stderr)) print(\"本次提交的urls:\", urls) # 提交到bing headers = { \"Content-Type\": \"application/json; charset=utf-8\", \"Host\": \"ssl.bing.com\", } data = {\"siteUrl\": \"your-site.com\", \"urlList\": urls} response = requests.post( url=\"https://www.bing.com/webmaster/api.svc/json/SubmitUrlbatch?apikey=your-key\", headers=headers, data=json.dumps(data) ) print(\"bing的响应: \", response.content) # 提交到百度 headers = { \"User-Agent\": \"curl/7.12.1\", \"Host\": \"data.zz.baidu.com\", \"Content-Type\": \"text/plain\" } response = requests.post( url=\"http://data.zz.baidu.com/urls?site=your-site.com\u0026token=your-token\", headers=headers, data=\"\\n\".join(urls) ) print(\"百度的响应: \", response.content) 运行结果： \u003e python commit_urls.py 将最新的url提交到百度和bing Switched to branch 'gh-pages' Your branch is up to date with 'origin/gh-pages'. 本次提交的urls: ['https://whuwangyong.github.io/2022-03-29-github-submodule/', 'https://whuwangyong.github.io/2022-04-27-linux-cpu-benchmark/'] bing的响应: b'{\"d\":null}' 百度的响应: b'{\"remain\":2998,\"success\":2}' ","date":"2022-04-28","objectID":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/:4:0","tags":["github pages","sitemap","webmaster"],"title":"使用API提交博客文章的URL到百度和Bing","uri":"/2022-04-28-submit-urls-to-baidu-and-bing-with-api/"},{"categories":["小工具"],"content":"场景 公司内网有maven仓库，扫描之后发现很多组件有漏洞，主要是因为版本太老。因此需要将这些漏洞组件的最新版导入内部的maven私服。 ","date":"2022-04-28","objectID":"/2022-04-28-crawl-latest-version-for-jar-npm-go-pkg/:1:0","tags":["python","package","maven"],"title":"使用python爬取jar包、npm包、go包的最新版本","uri":"/2022-04-28-crawl-latest-version-for-jar-npm-go-pkg/"},{"categories":["小工具"],"content":"问题 这么多漏洞组件，一个个去中央仓库找最新版，显然不科学。因此要整个脚本来做这件事情。 ","date":"2022-04-28","objectID":"/2022-04-28-crawl-latest-version-for-jar-npm-go-pkg/:2:0","tags":["python","package","maven"],"title":"使用python爬取jar包、npm包、go包的最新版本","uri":"/2022-04-28-crawl-latest-version-for-jar-npm-go-pkg/"},{"categories":["小工具"],"content":"方案 众所周知，jar包的中央仓库是https://mvnrepository.com/，但是这个网站有反爬虫机制，使用脚本发出的请求不会得到响应结果。替代方案是Maven Central Repository Search，这也是官方提供的，“Official search by the maintainers of Maven Central Repository”。更令人激动的是，这个网站提供了REST API！不用你费尽心机的去解析网页，人家直接给你接口。 npm包，访问\"https://registry.npmjs.org/{npm_package}/latest\"，从该页面可以抓取最新版。 go module，访问\"https://pkg.go.dev/{go_pkg }?tab=versions\"，从该页面可以抓取最新版。 这就万事大吉了吗？并不！这里有个大坑：什么是最新版本？上面这些网站按照时间戳排序返回的最新版，不一定是我们需要的。比如4.0.0-RC、2.3.18、3.2.53个版本，按时间排序，不稳定的4.0.0-RC和太老的2.3.18排在了前面。但我们需要的是3.2.5版本。这就涉及到版本号排序算法。关于版本号排序，python官方是支持的。但支持有限：（1）不能处理超过3位的版本号，1.2.3.4这种会报错；（2）不能处理字母，如2.12.2-Final、2.12.RELEASE。因此，需要做一些额外处理，将满足需求的最新版本摘出来。这个有点难，有些开发者提供的包版本号命名不规范，特别长的、带日期的、带特殊字符的、alpha、beta的等等都有。 ","date":"2022-04-28","objectID":"/2022-04-28-crawl-latest-version-for-jar-npm-go-pkg/:3:0","tags":["python","package","maven"],"title":"使用python爬取jar包、npm包、go包的最新版本","uri":"/2022-04-28-crawl-latest-version-for-jar-npm-go-pkg/"},{"categories":["小工具"],"content":"代码 可能需要的一些包： import math import re import traceback from typing import Dict, List import requests from bs4 import BeautifulSoup from distutils.version import StrictVersion from natsort import natsorted from soupsieve import match 获取jar包的最新版本： def process_maven(group_id, artifact_id): try: url = \"https://search.maven.org/solrsearch/select?q=g:{}a:{}\u0026core=gav\u0026rows=5\u0026wt=json\".format(group_id, artifact_id) response = requests.get(url) json = response.json() docs = json[\"response\"][\"docs\"] versions = [] for doc in docs: version = str(doc[\"v\"]) if (\"alpha\" in version.lower() or \"beta\" in version.lower() or \"dev\" in version.lower() or \"rc\" in version.lower() ): continue else: versions.append(version) if len(versions) == 0: print(\"没有找到符合要求的版本，g={}, a={}\".format(group_id, artifact_id)) continue latest_version = get_lastest_version(versions) return latest_version except Exception as e: traceback.print_exc() print(\"error: %s. artifact_id=%s.\" % (e, artifact_id)) print(\"response:\", response) def get_lastest_version(versions: List[str]) -\u003e str: d = dict() for version in versions: # version=4.3.10-RELEASE # 这种做法有问题 1.1.73.android: invalid version number '1.1.73.0000000' # fix_version = re.sub(r\"[a-zA-Z-]\", \"0\", version) # 最多只取前三位 1.2.3.4 -\u003e 1.2.3 fix_version = version if version.count(\".\") \u003e 2: i = version.rfind(\".\") fix_version = version[:i] # 1.2.Final -\u003e 1.2. a = re.findall(r\"[0-9.]\", fix_version) # 1.2. -\u003e 1.2 if a[-1] == \".\": a.pop() fix_version = \"\".join(a) # 排除掉以日期命名的版本号，这种版本号不正式，如 20030418.083655 if len(fix_version.split(\".\")[0]) \u003e= 8: continue # 31.1-jre 31.1-android if fix_version not in d: d[fix_version] = version l = sorted(d, key=StrictVersion) return d[l[-1]] 获取npm包的最新版本： def process_npm(npm): try: response = requests.get(url=\"https://registry.npmjs.org/\" + npm + \"/latest\").json() version = response[\"version\"] return version except Exception as e: print(\"error: %s. npm=%s.\" % (e, npm)) print(\"response:\", response) 获取go module的最新版本： def process_go(go_pkg): try: response = requests.get(url=\"https://pkg.go.dev/\" + go_pkg + \"?tab=versions\").content soup = BeautifulSoup(response, \"html.parser\") version_str = soup.find(\"a\", class_=\"js-versionLink\") version = version_str.contents[0] return version except Exception as e: print(\"error: %s. go_pkg=%s.\" % (e, go_pkg)) print(\"response:\", response) ","date":"2022-04-28","objectID":"/2022-04-28-crawl-latest-version-for-jar-npm-go-pkg/:4:0","tags":["python","package","maven"],"title":"使用python爬取jar包、npm包、go包的最新版本","uri":"/2022-04-28-crawl-latest-version-for-jar-npm-go-pkg/"},{"categories":["linux","性能测试"],"content":"sysbench Sysbench is mainly intended for doing database benchmarking. However it includes options to test CPU, memory and file throughput as well. ","date":"2022-04-27","objectID":"/2022-04-27-linux-cpu-benchmark/:1:0","tags":["sysbench","7z"],"title":"Linux CPU 性能测试","uri":"/2022-04-27-linux-cpu-benchmark/"},{"categories":["linux","性能测试"],"content":"安装 sudo apt install sysbench ","date":"2022-04-27","objectID":"/2022-04-27-linux-cpu-benchmark/:1:1","tags":["sysbench","7z"],"title":"Linux CPU 性能测试","uri":"/2022-04-27-linux-cpu-benchmark/"},{"categories":["linux","性能测试"],"content":"使用 ubuntu@instance:~$ sysbench cpu --threads=3 run sysbench 1.0.18 (using system LuaJIT 2.1.0-beta3) Running the test with following options: Number of threads: 3 Initializing random number generator from current time Prime numbers limit: 10000 Initializing worker threads... Threads started! CPU speed: events per second: 10519.03 General statistics: total time: 10.0003s total number of events: 105208 Latency (ms): min: 0.28 avg: 0.28 max: 5.22 95th percentile: 0.29 sum: 29976.02 Threads fairness: events (avg/stddev): 35069.3333/81.99 execution time (avg/stddev): 9.9920/0.00 events per second，值越大，性能越强。 上面是一个Oracle主机的测试结果，3个OCPU，每秒事件10519。单个OCPU每秒事件数为3484。i7-7700 CPU，单核每秒事件数1438，8核每秒事件数8469。可见Oracle主机的OCPU性能很强，单核性能是 i7-7700 的2.4倍。 ","date":"2022-04-27","objectID":"/2022-04-27-linux-cpu-benchmark/:1:2","tags":["sysbench","7z"],"title":"Linux CPU 性能测试","uri":"/2022-04-27-linux-cpu-benchmark/"},{"categories":["linux","性能测试"],"content":"7z 7z是个压缩/解压工具，压缩/解压天然吃CPU，用来测试性能顺理成章。而且官方还给出了一些CPU型号的测试结果排名：7-Zip LZMA Benchmark (7-cpu.com) ","date":"2022-04-27","objectID":"/2022-04-27-linux-cpu-benchmark/:2:0","tags":["sysbench","7z"],"title":"Linux CPU 性能测试","uri":"/2022-04-27-linux-cpu-benchmark/"},{"categories":["linux","性能测试"],"content":"安装 sudo apt install p7zip-full ","date":"2022-04-27","objectID":"/2022-04-27-linux-cpu-benchmark/:2:1","tags":["sysbench","7z"],"title":"Linux CPU 性能测试","uri":"/2022-04-27-linux-cpu-benchmark/"},{"categories":["linux","性能测试"],"content":"使用 语法： 7z b [number_of_iterations] [-mmt{N}] [-md{N}] [-mm={Method}] number_of_iterations 迭代次数，测试多少轮，可用于检查内存错误 mmt 线程数 举例： # 单线程 $ 7z b -mmt1 # 多线程 $ 7z b 结果说明： ubuntu@instance:~$ 7z b 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,3 CPUs LE) LE CPU Freq: - - - - - - 512000000 - - RAM size: 17954 MB, # CPU hardware threads: 3 RAM usage: 441 MB, # Benchmark threads: 3 Compressing | Decompressing Dict Speed Usage R/U Rating | Speed Usage R/U Rating KiB/s % MIPS MIPS | KiB/s % MIPS MIPS 22: 8642 175 4813 8407 | 98748 199 4227 8431 23: 8612 187 4695 8775 | 97477 200 4228 8438 24: 8438 196 4622 9073 | 95682 200 4201 8400 25: 8151 198 4694 9307 | 93742 200 4176 8344 ---------------------------------- | ------------------------------ Avr: 189 4706 8891 | 200 4208 8403 Tot: 194 4457 8647 上面是Orace OCPU的测试结果，单核4706。i7-7700单核3768。 Dict：字典大小，22表示2^21=4MB Usage：cpu总利用率。我有3个核，这里最多只用到200%，这是因为7z似乎只能使用2N个核（来源“When you specify (N*2) threads for test, the program creates N copies of LZMA encoder, and each LZMA encoder instance compresses separated block of test data.”——7-Zip LZMA Benchmark (7-cpu.com)） **MIPS：**million instructions per second R/U MIPS：单核性能。 Rating MIPS：约等于Usage * (R/U MIPS) 更多介绍，查看帮助文档。man 7z，在最下面会看到： HTML Documentation /usr/share/doc/p7zip-full/DOC/MANUAL/start.htm 用vim或者浏览器打开此页面进入帮助文档主页。或直接打开以下页面查看benchmark相关内容： chrome /usr/share/doc/p7zip/DOC/MANUAL/cmdline/commands/bench.htm ","date":"2022-04-27","objectID":"/2022-04-27-linux-cpu-benchmark/:2:2","tags":["sysbench","7z"],"title":"Linux CPU 性能测试","uri":"/2022-04-27-linux-cpu-benchmark/"},{"categories":["linux","性能测试"],"content":"数值运算 比如写个python脚本： import math import time t0 = time.time() for i in range(0, 10000000): math.pow(47,39) print(time.time() - t0) 将该脚本在不同的CPU上执行，通过对比运行时间，估计待测CPU的性能。 ","date":"2022-04-27","objectID":"/2022-04-27-linux-cpu-benchmark/:3:0","tags":["sysbench","7z"],"title":"Linux CPU 性能测试","uri":"/2022-04-27-linux-cpu-benchmark/"},{"categories":["静态网站博客"],"content":" 最好的参考资料仍然是官方。本文仅作一个基本描述 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:0:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"安装 Hugo 在此处下载安装包。有两个版本：（1）hugo；（2）hugo_extended。怎么选？很多功能，包括一些主题，都需要 hugo_extended 的支持，因此，建议安装 hugo_extended。下载之后，解压，将 hugo.exe 加入环境变量即可。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:1:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"创建站点 仅需一个命令： \u003e hugo new site my_blog 站点的目录结构如下： \u003e ls my_blog Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 2022/4/16 16:32 archetypes d----- 2022/4/16 16:32 content d----- 2022/4/16 16:32 data d----- 2022/4/16 16:32 layouts d----- 2022/4/16 16:32 static d----- 2022/4/16 16:32 themes -a---- 2022/4/16 16:32 82 config.toml 最关键的几个文件/目录: config.toml 配置文件，要定制化的东西几乎全在这里修改。 themes 存放主题的目录。里面可以放一个或多个主题 content 存放博客的目录。后续以 markdown 格式写的文章，就放在这下面： content/ └── posts/ └── this-is-my-first-blog/ \u003c-- page bundle | ├── index.md | └── sunset.jpg \u003c-- page resource └—— this-is-my-second-blog.md 注意，如果将 md 文件和引用的图片放在一个文件夹下（官方叫 page bundle），则 md 文件需命令为 index.md，否则 md 在渲染为 html 后，里面的图片不会显示。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:2:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"添加主题 新建的站点是空的，需要添加一个主题后才能正常使用。此处以 FixIt 主题为例。关于主题的选择，请看下一节主题推荐。 添加主题有两种方式： （1）将主题下载下来，放在 themes 目录下即可 （2）将主题以子模块的形式添加到站点，使用 git 管理。这么做主要是便于以后升级。本文采用此方式。关于 git submodule 的更多信息查看这里：git submodule - 标签 - 武大路飞 (whuwangyong.github.io) 另外，建议不要直接修改主题里面的文件，以后升级时合并起来很麻烦。没什么问题，主题可以一直使用，没必要频繁升级。 \u003e cd my_blog \u003e git init \u003e git submodule add https://github.com/Lruihao/FixIt.git themes/FixIt # 以后可以使用以下命令升级主题 \u003e git submodule update --remote --merge \u003e git add . \u003e git commit -m \"upgrade theme\" \u003e git push ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:3:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"主题推荐 官网提供了很多主题，我试用了一些，从以下几个角度进行选择： 用的人多，有人维护 侧边栏具备 Markdown 大纲目录 具备评论功能 具备搜索功能 简洁，美观，易用 最后选出了以下几个。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:4:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"Stack Stack | Hugo Themes (gohugo.io) 特点： 搜索很快 首页和正文的间距都很大 博客无修改时间 分类与标签的样式是一样的 favicon 图标设置：放在 hugo-theme-stack/static/img/ 目录下，修改 hugo-theme-stack/config.yaml，设置 params.favicon 为 /img/your-favicon.ico，注意是 /img 不是 img md 图片目录不能以 · 开始。否则渲染之后图片 src=\"/“是从根路径开始的，就找不到图片 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:4:1","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"Bootstrap Bootstrap Theme for Personal Blog and Documentations | Hugo Themes (gohugo.io) 这个主题的特点是，默认采用的 posts layout，这个布局下面的文章，侧边栏的 TOC 目录是不固定的。如果需要固定，请使用 docs layout。 This theme provides several kinds of layouts, such as posts and docs. Our documentations uses the docs layout. If you’re looking for an example that using posts layout, please take a look at Markdown Syntax. ——from：Docs Layout - Hugo Bootstrap (razonyang.com) 优点 页面控件支持超宽布局 代码控件支持超长代码折叠 Docs Layout 可以方便的将整个知识库放上去，这样本地的分类目录就能直接给博客使用，博客无需关心分类、标签的问题。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:4:2","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"​LoveIt dillonzq/LoveIt: ❤️A clean, elegant but advanced blog theme for Hugo 一个简洁、优雅且高效的 Hugo 主题 (github.com) 搜索 LoveIt 主题支持\"lunr\"和\"algolia\"两种搜索： lunr： 简单，配置 type = \"lunr\" 即可。运行 hugo 会将生成的 index.json 索引文件放在 public/ 目录下，随网站一起发布。没有 contentLength 的限制，但占用带宽大且性能低 (特别是中文需要一个较大的分词依赖库)。客户端需将整个 index.json 从网站下载到本地，然后基于此文件进行搜索。下图是使用 lunr 搜索时，生成的静态文件，可见分词库有 3.6MB： ​algolia：高性能并且占用带宽低，但需要将 index.json 上传到 algolia 官网（手动或使用 Algolia Atomic 脚本）；有 contentLength 的限制。对于免费用户：Your first 10,000 records are free, and every month you’ll receive 10,000 requests for our Search and Recommend products. 经过测试，lunr 导致网站加载速度变慢，且搜索效果很不理想。所以我选择了 algolia。配置如下：注意 index = \"new-index-1649076215\"，后面的值是你在 algolia 网站上创建的索引名。 [params.search]enable = true# 搜索引擎的类型 (\"lunr\", \"algolia\")type = \"algolia\"# 文章内容最长索引长度contentLength = 4000# 搜索框的占位提示语placeholder = \"\"# 最大结果数目maxResultLength = 10# 结果内容片段长度snippetLength = 50# 搜索结果中高亮部分的 HTML 标签highlightTag = \"em\"# 是否在搜索索引中使用基于 baseURL 的绝对路径absoluteURL = false[params.search.algolia]# 这里填写你在algolia上面创建的索引名index = \"new-index-1649076215\"appID = \"YMLXXXXFHL\"searchKey = \"9028b251fe4eexxxxxxxxxxxxx5a4f0\" 多语言 所有写在 [languages] 外面的，都是所有语言公用的。Multilingual Mode | Hugo (gohugo.io)。我去故意掉了多语言，只保留了中文：将 [languages.zh-cn] 下面的所有配置挪到外面，然后删除空的 [languages] 块。 使用本地资源 有三种方法来引用图片和音乐等本地资源: 使用页面包中的页面资源. 你可以使用适用于 Resources.GetMatch 的值或者直接使用相对于当前页面目录的文件路径来引用页面资源。所谓的页面包，就是图片和 md 文件放在一起（使用相对路径访问）。 将本地资源放在 assets 目录中, 默认路径是 /assets. 引用资源的文件路径是相对于 assets 目录的. 将本地资源放在 static 目录中, 默认路径是 /static. 引用资源的文件路径是相对于 static 目录的. 引用的优先级符合以上的顺序. 在这个主题中的很多地方可以使用上面的本地资源引用, 例如 链接 , 图片 , image shortcode, music shortcode 和前置参数中的部分参数. 页面资源或者 assets 目录中的图片处理会在未来的版本中得到支持. 非常酷的功能! Front Matter https://hugoloveit.com/zh-cn/theme-documentation-content/#front-matter 转义字符 https://hugoloveit.com/zh-cn/theme-documentation-content/#escape-character SRI 启用之后在 github.io 有问题： Failed to find a valid digest in the 'integrity' attribute for resource 'https://whuwangyong.github.io/lib/lunr/lunr.stemmer.support.min.d73a55668c9df0f2cbb2b14c7d57d14b50f71837e9d511144b75347e84c12ff8.js' with computed SHA-256 integrity 'EVRhgSylsJP5vMLxXSaTpskOj+ONq/I3Xl8Y4cNI2Xw='. The resource has been blocked. whuwangyong.github.io/:1 Failed to find a valid digest in the 'integrity' attribute for resource 'https://whuwangyong.github.io/lib/lunr/lunr.zh.min.e9abb2f5c7c0f738290cd8a5ff2ce1cf5deac6942f44ce5dd89c9ab1ae27006a.js' with computed SHA-256 integrity 's6qyS9abdG0o9DP0qC7PoVVqdbpe+fTKorzHq40yfBQ='. The resource has been blocked. whuwangyong.github.io/:1 Failed to find a valid digest in the 'integrity' attribute for resource 'https://whuwangyong.github.io/js/theme.min.09729ab43fbb7b49c065c597d41bb70983c7852ea77794a00b8f78099b049b43.js' with computed SHA-256 integrity '9Rk48wZaQO6EG8tVjkMw4x/SbA6lU0P/+HcLiLAxmjw='. The resource has been blocked. 页面现象就是侧边栏目录、评论都不显示。但是在 vercel.app 没问题。另外，console 还有个 warning：Error with Permissions-Policy header: Unrecognized feature: ‘interest-cohort’. LoveIt 总结 优点 文档很详细 默认在新标签页打开链接 可以设置代码超过 n 行折叠 标题加粗，更加清晰 使用 weight 置顶 ---weight:1# 置顶title:\"主题文档 - 基本概念\"date:2020-03-06T21:40:32+08:00lastmod:2020-03-06T21:40:32+08:00draft:falseauthor:\"Dillon\"authorLink:\"https://dillonzq.com\"description:\"探索 Hugo - LoveIt 主题的全部内容和背后的核心概念.\"resources:- name:\"featured-image\"src:\"featured-image.jpg\"tags:[\"installation\",\"configuration\"]categories:[\"documentation\"]lightgallery:truetoc:auto:false--- 缺点 搜索没有 jekyll + chirpy 的好，也没有 stack 的开箱即用。（algolia 要自己搞上传，lunr 分词有问题）。 主题不太美观 画廊。使用以下语法。不加“图片描述”不会激活。未使用画廊的图片，点击不会响应，只能右键新标签打开，放大查看。还有个小 bug，页面刷新后，图变小了。 ![image.png](assets/image-20220329160821-l8rh3to.png \"图片描述\") 最重要的，两年没维护了。但这个主题确实不错，所有有不少人 fork 了一份继续维护。我选择的是 Lruihao/FixIt。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:4:3","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"FixIt Lruihao/FixIt: 🔧 A clean, elegant but advanced blog theme for Hugo 一个简洁、优雅且高效的 Hugo 主题 (github.com) 它的原型基于 LoveIt 主题, LeaveIt 主题 和 KeepIt 主题。 LoveIt 主题 对我们来说是一个很棒的 Hugo 主题，很抱歉的是它的存储库已经停止维护很长一段时间了，所以我重建了一个名为 FixIt 的新主题，这样我可以更好地 Fix It 并使它用户体验更好。 修改了高亮颜色，比 LoveIt 更素雅好看一些。LoveIt 的橙色行内代码太花了 可以更便捷的修改页面宽度，LoveIt 的页面略窄 但是，图片刷新之后变小的 bug 还没解决 几乎可以从 LoveIt 无缝迁移 其他的就去看官方文档吧。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:4:4","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"Echo https://github.com/forecho/hugo-theme-echo 这个主题未体验，看了下觉得还不错，也列在这里吧。主要是到后面不想再折腾了…… ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:4:5","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"关于主题选择的总结 我最后的选择是 FixIt——LoveIt 的继续维护版。 其实我最喜欢的主题是 Chirpy，但这是 Jekyll 的主题。而 Jekyll 使用的 Kramdown 有问题，我做了很多尝试也无法解决，所以放弃了 Jekyll， 转 Hugo。 没有完美的主题，选择一个基本满足要求的即可。比起不断折腾主题，抓紧时间学习并输出优质内容更重要。另外，如果正在使用的主题有什么缺陷，首先应该仔细阅读官方文档和 issue 列表，寻找解决方案（这是一种能力），而不是立即去找一个新的主题代替它。因为，可能在换了主题之后，我发现新的主题在其他地方也有缺陷，最终落入“主题大师”的陷阱——在 N 个主题里面反复横跳，流于表面，没有任何定制或者解决问题的能力。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:4:6","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"运行示例站点 添加主题后，主题一般都带有示例站点，在 exampleSite 目录下。将 exampleSite 目录下的所有文件拷贝到站点目录下（my_blog）。然后使用如下命令启动： hugo server 访问 localhost:1313 即可查看效果。 如果报错“Twitter timeout”之类的，是因为示例站点里面有些 shortcode 会连接 twitter/YouTube 之类的东西，国内连不上。删掉相应文件即可。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:5:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"自定义配置 配置文件是 config.toml 或 config.yaml，有详细注释。另外，演示站点一般也是主题的说明文档，有不明白的配置项，可以在演示站点上查阅，很方便。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:6:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"新建文章 直接在 content/posts 目录下新建 xxx.md 即可。或者使用 page bundle 模式，将 index.md 和引用的图片放在同一文件夹。 也可以使用 hugo new posts 命令新建，posts 来源于主题提供的模板。如 FixIt 主题提供了以下模板： \u003e ls .\\themes\\FixIt\\archetypes\\ Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 2022/4/14 21:02 post-bundle -a---- 2022/4/14 21:02 151 default.md -a---- 2022/4/14 21:02 1044 friends.md -a---- 2022/4/14 21:02 179 offline.md -a---- 2022/4/14 21:02 633 posts.md ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:7:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"渲染 在站点目录下运行 hugo 命令即可。渲染之后的静态文件位于 public 目录下。将该目录下的所有文件放在一个 http 服务器下面，即可提供服务。比如，在 public 目录下，使用 python 命令运行一个 http 服务器： \u003e python -m http.server Serving HTTP on :: port 8000 (http://[::]:8000/) ... 然后浏览器访问 localhost:8000 即查看该站点。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:8:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"发布到 github pages 将上述渲染的结果——public 目录下的所有文件，提交到 username.github.io 这个 repo，即可发布到 github pages。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:9:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"进阶内容 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:10:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"发布到 netlify、vercel netlify、vercel 支持编译 hugo 源文件。因此，你可以直接提交 my_blog 下面的 hugo 源文件（包括你写的 md 文件、hugo 相关的配置、主题文件等等，不包括渲染后的 public/和渲染时生成的 resources/）到 github 的一个 repo，然后将该 repo 关联到 netlify、vercel，它们将会自动渲染并发布到它们的网站下。本站有相关文章，可以查看 netlify、vercel 标签。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:10:1","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"脚本化处理 我写了一个 python 脚本来做渲染、发布等事情，供参考。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:10:2","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"提交到 Google/百度/Bing 等搜索引擎 可以使用在 config.toml 中填写对应的配置；也可以将 Google/百度等提供的验证 html 文件放在站点的 static 目录下。渲染后，这些验证 html 文件会出现在 public/目录下。public/发布之后，它们就位于网站的根目录了，搜索引擎来抓取的时候就可以验证。 同样位于根目录的还有 sitemap.xml，这是网站地图，便于搜索引起爬取内容。另外，百度/Bing 还提供了提交 url 地址的 api。当你发布新文章后，可以手动或写脚本将 url 提交到搜索引擎，使文章更快地被收录。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:10:3","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"Tips ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:11:0","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"文章需要通过文件夹进行分类吗 在写了一些文章后，自然诞生出分类的想法。比如建站相关的，放在“建站”文件夹下；kafka 相关的，放在“kafka”目录下。 ​我的建议是不要这样做。因为分类是一个很难的事情，随着时间推移大概率会动态调整。调整之后意味着之前发布的博客的 url 失效。这对于 SEO 是很不利的，好不容易有个用户搜到了你的博客，一点进来却是 404。 类似的，posts 目录下的文件名、目录名，一经发布就不要改动。文章的标题和分类可以通过 Front Matter 修改： ---title:\"使用Jekyll + Github Pages搭建静态网站\"date:2022-03-29tags:[\"jekyll\",\"kramdown\",\"github pages\"]categories:[\"静态网站博客\"]--- ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:11:1","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"hugo 时区问题导致文章未显示 比如现在时间是 2022-04-19 0:56，我要发一篇文章，Front Matter 写为： ---title:\"使用 Hugo + Github Pages 创建静态网站博客\"date:2022-04-19tags:[\"hugo\"]--- date 字段我一般只写日期，不写时间。好，现在问题来了，hugo server 一把，发现该文章未显示。这是因为 hugo 默认时区比中国时间慢 8 小时，当前还是 4 月 18 日。解决办法有 4 个： 将 date 字段写详细：date: 2022-04-19T00:56:00+08:00 修改 config.toml，添加一行配置，执行可以编译未来的文章：buildfuture = true 使用 hugo server --buildFuture 或 hugo --buildFuture 命令 修改 config.toml，添加一行配置，指定时区：timezone = \"Asia/Shanghai\" 推荐采用方法（4）。 ","date":"2022-04-19","objectID":"/2022-04-19-hugo/:11:2","tags":["hugo","github pages","vercel","netlify"],"title":"使用 Hugo + Github Pages 创建静态网站博客","uri":"/2022-04-19-hugo/"},{"categories":["静态网站博客"],"content":"关键字：disable Vercel bot; disable Preview Deployments Vercel是一个支持部署前端框架和静态网站的平台。你可以方便的把 Github Pages 上面的站点导入Vercel，这样做有两个好处： 有些地区有些时候，Vercel上面的站点比Github Pages上的访问速度快； Github Pages 屏蔽了百度的爬虫，因此你的博客不能被百度检索到。但 Vercel 是可以的。 使用了一段时间的Vercel，发现有两个地方需要调整。 ","date":"2022-03-31","objectID":"/2022-03-31-vercel-disable-comments-and-preview-deployments/:0:0","tags":["vercel"],"title":"Vercel关闭评论 禁止Preview Deployments","uri":"/2022-03-31-vercel-disable-comments-and-preview-deployments/"},{"categories":["静态网站博客"],"content":"关闭评论 每次提交后，vercel bot会在该commit下评论： 这造成了大量的github通知和邮件通知： 如何关闭？在站点根目录新增vercel.json文件，内容如下： { \"github\": { \"silent\": true } } ","date":"2022-03-31","objectID":"/2022-03-31-vercel-disable-comments-and-preview-deployments/:1:0","tags":["vercel"],"title":"Vercel关闭评论 禁止Preview Deployments","uri":"/2022-03-31-vercel-disable-comments-and-preview-deployments/"},{"categories":["静态网站博客"],"content":"禁止Preview Deployments 另一个需要关闭的就是Preview Deployments（与我而言）。我的站点没有预览的必要，都是直接build main分支然后上production。Preview Deployments 构建的是gh-pages分支，可能是我哪里没弄对，这个分支一直无法build成功。如下图，2小时过去了还没结束，然后我手动取消了。 因此，我禁止了Preview Deployments，直允许Production Deployments。操作如下： Project Settings -\u003e Git -\u003e Ignored Build Step，在 COMMAND 填入 [ \"$VERCEL_ENV\" != production ]。 ​","date":"2022-03-31","objectID":"/2022-03-31-vercel-disable-comments-and-preview-deployments/:2:0","tags":["vercel"],"title":"Vercel关闭评论 禁止Preview Deployments","uri":"/2022-03-31-vercel-disable-comments-and-preview-deployments/"},{"categories":["静态网站博客"],"content":"Reference How do I prevent the Vercel for GitHub integration comments? – Vercel Docs Using Vercel without preview deployments (codejam.info) ","date":"2022-03-31","objectID":"/2022-03-31-vercel-disable-comments-and-preview-deployments/:3:0","tags":["vercel"],"title":"Vercel关闭评论 禁止Preview Deployments","uri":"/2022-03-31-vercel-disable-comments-and-preview-deployments/"},{"categories":null,"content":"最近学习使用 Hugo 构建静态网站。安装主题时，接触到git submodule这个命令，踩了些坑，总结一下。 ","date":"2022-03-29","objectID":"/2022-03-29-github-submodule/:0:0","tags":["git submodule"],"title":"Github SubModule 指南","uri":"/2022-03-29-github-submodule/"},{"categories":null,"content":"子模块与父模块如何同步更新 子模块是一个单独的项目，commit push等操作需要在子模块自己的repo目录下操作。父项目的操作与子模块无关。父项目git add无法添加子模块的changes，当然也就无法commit push。 子模块版本升级后，父项目不会自动升级，仍然停留在对之前版本的引用。以下命令可以查看父模块当前使用子模块的哪个版本： \u003e git submodule status f0dc1cf84d7c47dc1625e956f07b37b6c238a3dc themes/hugo-theme-stack (v3.8.0-4-gf0dc1cf) 子模块修改后，父模块虽然无法git add/commit/push，但是git status却可以显示： \u003e git status On branch master Your branch is up to date with 'origin/master'. Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git restore ...\" to discard changes in working directory) modified: themes/hugo-theme-stack (new commits) 这里会让人疑惑，有changes，但是无法add进来。 这是因为，子模块的修改，只能在子模块里面进行commit \u0026 push。 子模块commit \u0026 push之后，父模块不会自动更新，仍然保持着对子模块上一版本的引用。此时可以使用git add submoduleDir 添加子模块的更新，然后commit \u0026 push，将子模块的修改同步到父模块。 如果父模块在尚未add \u0026 commit \u0026 push子模块更新的情况下，执行了git submoule update，此时子模块会回滚到上一版本（父模块引用的那个版本）。然后去子模块git status，会提示 HEAD detached at xxxxxx。 如果发生了上述情况，可以在子模块git branch -a查看分支，git checkout到上次提交修改的分支。然后子模块就恢复到working tree clean了。 再去父模块，git add submoduleDir \u0026\u0026 commit \u0026\u0026 push。 ","date":"2022-03-29","objectID":"/2022-03-29-github-submodule/:1:0","tags":["git submodule"],"title":"Github SubModule 指南","uri":"/2022-03-29-github-submodule/"},{"categories":null,"content":"无法拉取submodule代码到本地 git clone 了一个包含submodule的仓库，打开submodule目录，发现里面是空的： 可尝试以下命令： git submodule update git submodule update --recursive --remote git submodule update --init ","date":"2022-03-29","objectID":"/2022-03-29-github-submodule/:2:0","tags":["git submodule"],"title":"Github SubModule 指南","uri":"/2022-03-29-github-submodule/"},{"categories":null,"content":"fatal: could not get a repository handle for submodule xxx 这是很常见的错误。当上次 clone into 未完成就直接关闭，下次 git submodule update 就会报这个错。 $ git submodule update --init Cloning into 'D:/blog/whuwangyong.github.io/themes/FixIt'... ^C $ git submodule update BUG: submodule considered for cloning, doesn't need cloning any more? fatal: could not get a repository handle for submodule 'themes/FixIt' 解决办法：将 submodule 路径下的.git文件删掉即可。 ","date":"2022-03-29","objectID":"/2022-03-29-github-submodule/:3:0","tags":["git submodule"],"title":"Github SubModule 指南","uri":"/2022-03-29-github-submodule/"},{"categories":null,"content":"如何修改子模块的 remote url 需求：使用的别人的主题repo作为子模块。然后改了些地方，发现无法提交到父项目，更不能提交到别人的repo。 解决：fork别人的repo，然后使用自己fork的repo作为子模块。后面原作者的repo更新了，再单独升级fork后的repo。升级时注意别覆盖了自己的修改。如果使用良好，不升级也可。 那么，如何将submodule，从引用别人的repo，改为引用自己的。 ","date":"2022-03-29","objectID":"/2022-03-29-github-submodule/:4:0","tags":["git submodule"],"title":"Github SubModule 指南","uri":"/2022-03-29-github-submodule/"},{"categories":null,"content":"先删除 GIT 未提供submodule remove的功能。要删除一个子模块，需按照下面步骤操作： git submodule deinit sub/module，执行后模块目录将被清空。 git rm sub/module，执行后会清除.gitmodules里的项目。 git commit -m ‘remove sub/module。 第一步不做似乎也没关系。第二步是关键，这里不能直接rm sub/module，这样不会同步更新.gitmodules的内容。 ","date":"2022-03-29","objectID":"/2022-03-29-github-submodule/:4:1","tags":["git submodule"],"title":"Github SubModule 指南","uri":"/2022-03-29-github-submodule/"},{"categories":null,"content":"再添加 git submodule add https://github.com/whuwangyong/hugo-theme-stack/ themes/hugo-theme-stack 如果遇到如下错误： A git directory for ‘hugo-theme-stack’ is found locally with remote(s): origin https://github.com/CaiJimmy/hugo-theme-stack/ If you want to reuse this local git directory instead of cloning again from https://github.com/CaiJimmy/hugo-theme-stack/ use the ‘–force’ option. If the local git directory is not the correct repo or you are unsure what this means choose another name with the ‘–name’ option. 需删除本地的缓存： git rm --cached sub/module 或直接删除站点根目录的.git/module/下面对应的子模块： rm -rf .git/module/hugo-theme-stack 然后再次执行git submodule add。 ","date":"2022-03-29","objectID":"/2022-03-29-github-submodule/:4:2","tags":["git submodule"],"title":"Github SubModule 指南","uri":"/2022-03-29-github-submodule/"},{"categories":null,"content":"Reference GIT 的 SUBMODULE - 阅微堂 (zhiqiang.org) git submodule删除后重新添加问题_Week Mao的专栏-CSDN博客 来说说坑爹的 git submodule - 掘金 (juejin.cn) Git Submodule_弹吉他的小刘鸭的博客-CSDN博客 ","date":"2022-03-29","objectID":"/2022-03-29-github-submodule/:5:0","tags":["git submodule"],"title":"Github SubModule 指南","uri":"/2022-03-29-github-submodule/"},{"categories":["笔记软件"],"content":"这段时间试用了好多笔记软件，我本来就是个选择困难症，一个个试下来，愁死我了。 分享一下我的一些阶段性结论。 ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:0:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"有道 缺点： 编辑器有问题，Ctrl+Home/End无法跳转、table键无法切换单元格、光标移动慢等问题 富文本不能导出md（这是我想放弃它的唯一原因） 新旧版本笔记不兼容，所以里面有两个编辑器 加密文件夹里面的笔记居然可以通过搜索直接显示！ 有些bug简直是feature了，5年不修：关机时提示“内存无法read”的错误，导致PC关机失败。（最新重构后的7.x版本才解决这个问题） 修bug很慢。反馈了之后很久没变化 优点： 免费（我用的早，有25G空间了） 微信收藏 支持plantuml！ 稳定，没丢过数据 有道是我的主力笔记软件，2000多篇，800MB左右（所以3G空间其实也够用） ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:1:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"为知 缺点 表格插入列的时候，鼠标上的提示有一定几率显示为“插入行” WizNote X 现在 md 笔记不需要添加.md 后缀了，但是在 app 上，md 格式的笔记如果没有 md 后缀，会直接以 md 语法的文本显示，不会渲染。手动添加.md 后缀正常渲染，但是手动添加的这个后缀在 PC 上是看不到的 笔记的格式不统一。单独的WizLite写的 md 虽然可以同步到一个账号，但是跟直接在WizNote X里面的笔记不同。网页剪藏的md笔记，会提示升级为md，但它本来就是md啊。反正为知笔记的格式我一直没太搞清楚。 app无法点开图片（即无法放大看） 反馈不通畅。邮件、官网的兔小巢、贴吧都反馈了bug，没有回复 产品很好，但是让我觉得要被放弃了 优点： 60/年不贵 网页剪藏最厉害 微信收藏最厉害（有道需在app打开一下那个笔记才能收藏，否则内容就是个url。为知不用，微信发给公众号就行） 完美导入/导出md。完美指的是导入md时，能把图片上传到wiz服务器；导出时能把图片导出且和md文件同一文件夹 支持私有部署（私有部署不能使用网页收藏、微信收藏） ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:2:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"Joplin 缺点 编辑器不行；虽然可以调用外部 Typora 打开，但是插入的图片不在 Joplin 的数据库里面 有个大纲插件，但是简陋：每次打开都占1/2宽的屏幕，即使调整了也记不住位置 没有微信收藏（现在公众号还是很多优质内容的） 优点： 开源，稳定 可以导出md（但是其图片不是放在md文件同一目录，而是上级目录。所有md使用的图片混在一个目录） 支持私有部署（具备网页剪藏） ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:3:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"思源 缺点： 虽然版本更新很快，但是bug还是多（好在反馈通道很通畅） 单独说一下目前存在的隐藏大bug：容易触发“状态异常”，然后只能退出软件 网页剪藏慢，行为类似手机滚动截图，在浏览器里一屏一屏的滚 优点： 编辑器很强（支持plantuml），操作表格很方便，距离typora只差一点 搜索功能强大（可以SQL查询） 完美导入/导出md（图片在当前路径，这便于我用hugo构建静态网页） 支持整个笔记本导出md 支持双链和日记 支持私有部署（目前只有x86，arm的镜像还没有），私有部署具备网页剪藏功能 可玩性强。结合嵌入块、挂件、API等，能玩出花儿 ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:4:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"语雀 缺点： 导出md不能导出图片。官网明确说了不会支持，是为了防盗链 没有网页剪藏和微信收藏 优点： 小记是很好的功能 编辑器功能丰富（支持plantuml） ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:5:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"logseq 没有深度使用，可能有失公允。 优点： 反链很友好。logseq把反链全部放在文末，便于查看。思源是放在侧边栏的，就不太方便 支持层级页面，如[[PageA/PageB]] 缺点： 编辑器不行，比如表格操作这些差远了（第三方插件可能有相关支持，没研究）；然后所见即所得方面，logseq是以block（就是大纲的一个点）为单位的。比如一个300字的段落，里面有行内代码、超链接等等，在编辑这个段落时，行内代码、超链接等等都不会渲染 导入/导出md。logseq本身就是以md格式存储的，导入/导出就是手动拷贝md文件。不足的是，导出md时，md文件是拷贝出来了，但md引用的图片，却在一个大文件夹（assets）里面（所有笔记的附件都在这），只能根据文件名逐个去找 ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:6:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"Obsidian、Notion、我来等其他双链笔记 没有深度使用，不做评价。但是，根据我对双链笔记的理解来说，我不会选他们。理由见下方“推荐资料”。 ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:7:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"推荐资料 强烈推荐：双向链接时代的快速无压记录 · 语雀 (yuque.com) flomo - 思维方式 B站Up主“二一的笔记” 这些理念，影响了我对笔记软件的选择。 ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:8:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["笔记软件"],"content":"我目前的选择：思源 + flomo 导出MD是刚需。因为写了很多笔记之后，如果不发布出去，与别人分享交流，那意义不大。如果无法导出MD，发布时会很麻烦。 我目前正在使用思源免费版。电脑之间用 OneDrive 同步，速度快；手机上使用FolderSync，同步速度慢一点，可以接受。忽略历史文件的（history文件夹）同步会快很多。手机上基本就只是看，不会编辑，因此是单项同步。后面如果笔记多了导致同步太慢，考虑Docker部署。或者换一种同步方式，比如NextCloud。 ~~微信收藏还是用的有道。有道现在充当一个收集箱，绑定微信公众号之后，可以很方便的把文字、图片、文件等扔给有道暂存。~~微信收藏改用flomo。 为了选择一个顺手的工具，搭进去了好多的时间。看评测，做测试，还顺带着反馈bug。唉……没有完美的产品，放弃完美主义，找一个差不多能打的就行了（logseq、RemNote等都很不错），然后熟练使用，这是一个人与工具之间互相磨合的过程，好比剑士用剑。不要舍本逐末，工具只是辅助，坚持写更重要。 ","date":"2022-03-29","objectID":"/2022-03-08-notesapp-benchmark/:9:0","tags":["为知笔记","有道云笔记","思源笔记","logseq","双链笔记","obsidian","joplin","语雀"],"title":"云笔记软件简评","uri":"/2022-03-08-notesapp-benchmark/"},{"categories":["静态网站博客"],"content":"Jekyll 是 Github Pages 官方支持的静态网站生成工具，优点是在可以直接github上使用vscode online编辑md，提交后，github会承担生成html的工作。而使用hugo等工具，需要先在本地将md文件渲染成html，然后上传。 提示 hugo的优点是快！ 虽然github pages只支持渲染Jekyll，但是netlify、vercel等平台支持渲染hugo、jekyll等更多框架。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:0:0","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"主题选择 看了几个jekyll主题，发现 Chirpy 最得我心。在jekyll-template · GitHub Topics下，Chirpy主题排名第二。 本文记录Jekyll和chirpy的搭配使用。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:1:0","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"安装Ruby和Jekyll 前面说了github可以编译Jekyll，为什么还要在本地装一套环境呢。主要是为了方便调试，尤其是刚开始配置主题的时候。 比起Hugo来说，jekyll的安装要麻烦一些，需要安装的东西一大堆： Follow the instructions in the Jekyll Docs to complete the installation of Ruby, RubyGems, Jekyll, and Bundler. Download and install a Ruby+Devkit version from RubyInstaller Downloads. Use default options for installation. Run the ridk install step on the last stage of the installation wizard. This is needed for installing gems with native extensions. You can find additional information regarding this in the RubyInstaller Documentation 第一次接触ruby，完全懵逼，不知道装了些啥，接近1个GB。打印的日志是清新脱俗。 Open a new command prompt window from the start menu, so that changes to the PATH environment variable becomes effective. Install Jekyll and Bundler using gem install jekyll bundler Check if Jekyll has been installed properly: jekyll -v 到这里，本地环境就安装好了。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:2:0","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Chirpy主题的使用 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:0","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Install Creating a New Site 跟着这里操作。简言之，使用其提供的 template repo 创建自己的 repo，命名为\u003cGH_USERNAME\u003e.github.io。 Installing Dependencies 将创建的 repo（\u003cGH_USERNAME\u003e.github.io） clone 到本地，执行： git clone git@github.com:\u003cGH_USERNAME\u003e/\u003cGH_USERNAME\u003e.github.io cd \u003cGH_USERNAME\u003e.github.io bundle 依赖安装完后，生成一个Gemfile.lock文件。 \u003cGH_USERNAME\u003e.github.io里面有一个Gemfile文件，它指定了你想要使用的gem的位置和版本。 bundle命令根据Gemfile文件安装依赖，并将安装的每个依赖的版本，记录在Gemfile.lock文件里。这样，当相同的库或是项目在另一台机器上加载时，运行bundle install将安装完全相同的版本，而不是最新的版本。(在不同的机器上运行不同的版本可能会导致测试失败等等)。简单来说就是保证在不同环境下gem版本相同。 Gemfile.lock文件可以提交到github，让github pages的deploy 脚本也使用相同的版本。但是，由于我是在Windows上运行bundle的，github pages的部署环境是linux。因此，需要运行以下命令，将 x86_64-linux 平台的一些库添加到Gemfile.lock里面（只有几个，多数是跨平台兼容的）： bundle lock --add-platform x86_64-linux ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:1","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Configuration 主要就是修改如下几个文件： _config.yml timezone:Asia/Shanghaigoogle_site_verification:googledxxxxxxx.html# 去Google Search Console申请，用于google收录avatar:assets/img/avatar/wy-avatar-339x335.jpg# 头像comments:active:utterances# 使用github issue作为文章的评论系统utterances:repo:whuwangyong/whuwangyong.github.io # \u003cgh-username\u003e/\u003crepo\u003eissue_term:title # \u003c url | pathname | title | ...\u003epaginate:20 _tabs/about.md 自定义“关于我”的内容。 favicon 自定义网站图标。将favicon.ico文件放入assets/img/favicons/。Customize the Favicon - Chirpy (cotes.page) _data/share.yml 配置文章的分享选项，如Facebook、微博之类的。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:2","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Writing a New Post Writing a New Post | Chirpy (cotes.page) 在_posts目录下创建YYYY-MM-DD-TITLE.md文件即可，必须按照此格式命名。可以使用子目录，用于分类 图片必须放在/assets/下。最佳实践：放在/assets/img/posts/[YYYY-MM-DD-TITLE]目录下 Front Matter ---title:TITLEdate:YYYY-MM-DD HH:MM:SS +/-TTTT# 2022-01-01 13:14:15 +0800 只写日期也行；不写秒也行；这样也行 2022-03-09T00:55:42+08:00categories:[TOP_CATEGORIE, SUB_CATEGORIE]tags:[TAG] # TAG names should always be lowercaseauthor:# 不写默认就是自己name:Full Namelink:https://example.com# 以下默认falsemath:truemermaid:truepin:true--- 分类 支持多层级。如categories: [Java, Spring, AOP]，最终的分类效果是Java/Spring/AOP，这样就可以复用笔记软件里面设置好的分类。 标签 该主题的作者建议，TAG names should always be lowercase。我猜测这么做的原因是，大小写混用会导致相同含义的标签以多种形式出现，如：VSCode、VScode、vscode。学到了。在我的笔记软件里面，大小写混用的标签正让我苦不堪言。 img_path 当文中很多图片具备共同的前缀时，可以将该前缀提取出来，放在Front Matter。 Liquid Codes 举例：如果你在正文中添加如下内容，则会输出该文章的标题。 {{ page.title }} 更多参考：Liquid 模板语言中文文档 | Liquid 中文网 (bootcss.com) ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:3","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Running Local Server You may want to preview the site contents before publishing, so just run it by: bundle exec jekyll s # serve, server, s Serve your site locally # 编译命令是 bundle exec jekyll b # build, b Build your site After a while, the local service will be published at http://127.0.0.1:4000 . ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:4","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Deployment Deploy by Using Github Actions。直接提交到github即可。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:5","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Upgrading 如前文所述，依赖的库及其版本都指定在Gemfile 里面，因此，修改此文件，更新jekyll-theme-chirpy的版本号即可： - gem \"jekyll-theme-chirpy\", \"~\u003e 3.2\", \"\u003e= 3.2.1\" + gem \"jekyll-theme-chirpy\", \"~\u003e 3.3\", \"\u003e= 3.3.0\" And then execute the following command: bundle update jekyll-theme-chirpy As the version upgrades, the critical files (for details, see the Startup Template) and configuration options will change. Please refer to the Upgrade Guide to keep your repo’s files in sync with the latest version of the theme. ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:6","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"发布时间与更新时间 chirpy主题还有个优点：自带文章的更新时间。 ​这就不需要倒腾额外的jekyll插件去实现这个功能了。如gjtorikian/jekyll-last-modified-at: A Jekyll plugin to show the last_modified_at time of a post. (github.com) ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:7","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"添加tab到左侧栏 如新增“友情链接”tab。在_tabs目录下新建links.md: --- title: 友情链接 icon: fas fa-link order: 5 --- 调整order和icon。icon去Font Awesome Icons里面找。然后修改_data/locales/en.yml和_data/locales/zh-CN.yml，在tabs:下添加links: Links和links: 友链，以适配中英文。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:3:8","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"进阶内容 用了两天，对Jekyll + Github Pages 的工作逻辑有了一些理解。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:0","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"为什么使用Jekyll可以直接提交md 在\u003cgithub_username\u003e.github.io根目录下，查看.github/workflows/pages-deploy.yml文件： name:'Automatic build'on:push:branches:- mainpaths-ignore:- .gitignore- README.md- LICENSEjobs:continuous-delivery:runs-on:ubuntu-lateststeps:- name:Checkoutuses:actions/checkout@v2with:fetch-depth:0# for posts's lastmod- name:Setup Rubyuses:ruby/setup-ruby@v1with:ruby-version:2.7bundler-cache:true- name:Deployrun:bash tools/deploy.sh 该文件定义了一个workflow：当push代码到main分支时，执行jobs里面定义的动作。最关键的是Deploy这一步，它执行了一个脚本：tools/deploy.sh。这个脚本做的事情，就是执行bundle exec jekyll build -d _site将md文件编译为html，生成静态网站，然后将_site下的内容push到gh-pages分支。到这里就很清楚了，是github帮助我们执行了build操作，将md转换成了html。 在github上查看你的github.io项目，在Actions下面可以看到每次提交新文章时触发的workflows： ​第一个workflow就是上面提到的.github/workflows/pages-deploy.yml，第二个是github pages创建的，可以看到bot标志。点进workflow runs，可以看到执行的日志，根据日志能更加清楚的知道背后的流程。 如果使用hugo建站，github后台并没有hugo的环境，所以不能帮助我们编译md。这就需要我们自己编译好html，然后push到github.io项目。push之后的流程是一样的：由github pages的bot将编译好的静态网站发布到https://\u003cusername\u003e.github.io。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:1","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"自定义workflow 修改tools/deploy.sh，在里面加入自己的操作。如：将sitemap提交到百度资源平台。 在tools目录下新建脚本，然后在.github/workflows/pages-deploy.yml里面调用 在.github/workflows/下创建新的workflow 第一个亲测可行，后两个还没研究。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:2","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"提交到谷歌/百度等搜索引擎 修改tools/deploy.sh，在里面新增一个函数：在deploy之后，根据生成的 sitemap.xml 创建一个包含所有url的sitemap.txt文件。将该文件提交到百度资源平台。但是GitHub封了百度的爬虫，可以考虑在vercel上也部署一份，让百度去爬vercel。另一种方法是使用反向代理，让百度爬自己的主机，自己的主机去连接github。 至于google和bing，访问 Google Search Console 和 Bing Webmaster Tools 进行设置，添加博客地址之后就等着爬虫光临。 搜狗也可以试一试，GitHub没封搜狗。 # file_name: tools/deploy.sh # 新增 submit_sitemap 函数，在 deploy 之后调用 submit_sitemap() { echo \"------ \u003e\u003e\u003e submit_sitemap ---------\" grep \"\u003cloc\u003e\" sitemap.xml | grep -v 'html' | awk -F '[\u003c \u003e]' '{print $3}' \u003e sitemap.txt curl -H 'Content-Type:text/plain' --data-binary @sitemap.txt \"http://data.zz.baidu.com/urls?site=https://whuwangyong.github.io\u0026token=5os4wCK5ct7kBZRN\" curl -H 'Content-Type:text/plain' --data-binary @sitemap.txt \"http://data.zz.baidu.com/urls?site=https://whuwangyong.vercel.app\u0026token=5os4wCK5ct7kBZRN\" rm -f sitemap.txt echo \"------ submit_sitemap \u003c\u003c\u003c ---------\" } main() { init build # ... deploy submit_sitemap } ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:3","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"netlify、vercel是什么 除了github pages，还有netlify、vercel等也能生成并部署静态网站。它们从\u003cusername\u003e.github.io 的main分支拉取md源文件，然后在自己的资源上运行bundle jekyll exec build，将build之后的html放在它们自己的服务器上，并提供一个域名供你访问这个静态站点。 除了Jekyll，它们还支持Hugo、Hexo、Next.js等多种静态网站构建工具。也就是说，只将github作为一个代码托管平台，里面可以放Jekyll、Hugo等多种构建工具和md文件，netlify和vercel都可以将它们编译为html并发布出去。从这个方面说，它们比github强大。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:4","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Jekyll的Markdown处理器 Jekyll默认的Markdown Processor是kramdown，这与我们常使用的typora、vscode的md解析器不同。kramdown会将pipe字符|解析为table语法。如果正文中有link或者image语法，且文本包含了|字符，需要对其进行转义。下述这种写在代码块里面可以，如果写在正文，就会当成表格了。 [A | B](http://foo.com) [img | 1](http://img.com/1.jpg) 这个问题2014年就有人提了，但是作者一直没修复： Pipes inside image alt text lead to unwanted table output · Issue #135 · gettalong/kramdown (github.com) the conditional probability equation doesn’t display normal · Issue #46 · gettalong/kramdown (github.com) Kramdown bug - pipe character in Markdown link creates a table · Issue #2818 · jekyll/jekyll (github.com) markdown - How do you disable tables in Kramdown? - Stack Overflow 对 Markdown 渲染引擎 kramdown 的几点 hack | 明无梦 (dreamxu.com) Table syntax · Issue #151 · gettalong/kramdown (github.com) Pipe characters in text creates table · Issue #317 · gettalong/kramdown (github.com) Bug - pipe character in Markdown link creates a table · Issue #431 · gettalong/kramdown (github.com) 这是2017年的issue，作者回复说2.x版本会修复，但仍然存在。在kramdown - syntax页面，搜索“Automatic and Manual Escaping”可以查看kramdown所有（可能）需要转义的字符。 所以，如果要继续用kramdown，要么禁用table语法，要么把所有用到|的地方全部转义。这两个我都不会选：不用table不可能；为了适应kramdown修改标准的md语法更不可能。 除了|字符，\u003c\u003e、liquid cldoe语法（{{}}）等也需要转义： jekyll kramdown渲染的htmlimage.png \"\rjekyll kramdown渲染的html\r hugo渲染的htmlimage.png \"\rhugo渲染的html\r jekyll kramdown渲染的htmlimage.png \"\rjekyll kramdown渲染的html\r hugo渲染的htmlimage.png \"\rhugo渲染的html\r 总之要注意的地方挺多，不能毫无顾忌地写markdown。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:5","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"更改Jekyll的markdown处理器 鉴于kramdown的上述问题，我尝试给Jekyll换一个Markdown处理器。根据文档Markdown Options - Jekyll，除了kramdown，Jekyll还支持jekyll-commonmark和jekyll-commonmark-ghpages。我分别试用了这两个处理器，问题更多。尤其是jekyll-commonmark-ghpages，其兼容的jekyll版本是3.9.x，与我使用的Chirpy主题（需要jekyll 4.x）不兼容。jekyll-commonmark倒是解决了|的问题，但是代码高亮有问题，有些代码始终无法渲染，花了整整一天翻遍了github也没找到jekyll-commonmark到底应该怎么配置。不负责任的说一句，这就是个坑。 Jekyll也支持自定义处理器，我没尝试。另一个优质主题jekyll-TeXt-theme同样用的kramdown。看来用Jekyll就避免不了kramdown。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:6","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"搜索功能 Chirpy主题的作者在这里提到，他使用的是Simple-Jekyll-Search实现的搜索功能。效果不错，速度飞快。 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:7","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"显示阅读量 使用Google Page Views： Enable Google Page Views | Chirpy (cotes.page) ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:4:8","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["静态网站博客"],"content":"Reference Getting Started | Chirpy (cotes.page) https://blog.csdn.net/qq_38534524/article/details/108462825 ","date":"2022-03-29","objectID":"/2022-03-29-jekyll/:5:0","tags":["jekyll","kramdown","github pages","vercel","netlify"],"title":"使用Jekyll + Github Pages搭建静态网站","uri":"/2022-03-29-jekyll/"},{"categories":["kafka"],"content":"简介 Apach Kafka 是一款分布式流处理框架，用于实时构建流处理应用。它有一个核心的功能广为人知，即作为企业级的消息引擎被广泛使用。 很多主流消息引擎系统都支持 JMS（Java Message Service）规范，比如 ActiveMQ、RabbitMQ、IBM 的 WebSphere MQ 和 Apache Kafka。Kafka 并未完全遵照 JMS 规范。 像 RabbitMQ 或 ActiveMQ 这样的传统消息中间件，它们处理和响应消息的方式是破坏性的（destructive），即一旦消息被成功处理，就会被从 Broker 上删除。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:1:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"集群部署 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:2:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"硬件 操作系统：Linux。IO模型；零拷贝 磁盘 磁盘容量 带宽 I/O 模型与 Kafka 的关系又是什么呢？实际上 Kafka 客户端底层使用了 Java 的 selector，selector 在 Linux 上的实现机制是 epoll，而在 Windows 平台上的实现机制是 select。因此在这一点上将 Kafka 部署在 Linux 上是有优势的，因为能够获得更高效的 I/O 性能。 主流的 I/O 模型通常有 5 种类型：阻塞式 I/O、非阻塞式 I/O、I/O 多路复用、信号驱动 I/O 和异步 I/O。每种 I/O 模型都有各自典型的使用场景，比如 Java 中 Socket 对象的阻塞模式和非阻塞模式就对应于前两种模型；而 Linux 中的系统调用 select 函数就属于 I/O 多路复用模型；大名鼎鼎的 epoll 系统调用则介于第三种和第四种模型之间；至于第五种模型，其实很少有 Linux 系统支持，反而是 Windows 系统提供了一个叫 IOCP 线程模型属于这一种。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:2:1","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"重要的参数配置 Broker参数 log.dirs=/home/kafka1,/home/kafka2,/home/kafka3 zookeeper.connect 多个 Kafka 集群使用同一套 ZooKeeper 集群，zookeeper.connect参数可以这样指定：zk1:2181,zk2:2181,zk3:2181/kafka1和zk1:2181,zk2:2181,zk3:2181/kafka2。切记 chroot 只需要写一次，而且是加到最后的。我经常碰到有人这样指定：zk1:2181/kafka1,zk2:2181/kafka2,zk3:2181/kafka3，这样的格式是不对的。 listeners：学名叫监听器，其实就是告诉外部连接者要通过什么协议访问指定主机名和端口开放的 Kafka 服务。 advertised.listeners：和 listeners 相比多了个 advertised。Advertised 的含义表示宣称的、公布的，就是说这组监听器是 Broker 用于对外发布的。 log.retention.hours=168 log.retention.bytes：这是指定 Broker 为消息保存的总磁盘容量大小 message.max.bytes：控制 Broker 能够接收的最大消息大小 Topic参数 retention.ms：规定了该 Topic 消息被保存的时长。默认是 7 天，即该 Topic 只保存最近 7 天的消息。一旦设置了这个值，它会覆盖掉 Broker 端的全局参数值。（**待验证。**oracle上面创建了kafka，broker参数为100天，topic删除为默认7天。今天生产了消息，7天后看是否删除消息。2022-03-27 00:52） max.message.bytes。它决定了 Kafka Broker 能够正常接收该 Topic 的最大消息大小。注意与broker的参数message.max.bytes不同。 JVM参数 heap size：6GB，这是目前业界比较公认的一个合理值。默认 1GB 有点小，毕竟 Kafka Broker 在与客户端进行交互时会在 JVM 堆上创建大量的 ByteBuffer 实例。 使用 G1 收集器。在没有任何调优的情况下，G1 表现得要比 CMS 出色，主要体现在更少的 Full GC，需要调整的参数更少。 $\u003e export KAFKA_HEAP_OPTS=--Xms6g --Xmx6g $\u003e export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true 操作系统参数 文件描述符限制：通常情况下将它设置成一个超大的值是合理的做法，比如ulimit -n 1000000，避免“Too many open files”的错误。 文件系统类型：XFS 的性能要强于 ext4。 Swappiness：网上很多文章都提到设置其为 0，将 swap 完全禁掉以防止 Kafka 进程使用 swap 空间。我个人反倒觉得还是不要设置成 0 比较好，我们可以设置成一个较小的值。为什么呢？因为一旦设置成 0，当物理内存耗尽时，操作系统会触发 OOM killer 这个组件，它会随机挑选一个进程然后 kill 掉，即根本不给用户任何的预警。但如果设置成一个比较小的值，当开始使用 swap 空间时，你至少能够观测到 Broker 性能开始出现急剧下降，从而给你进一步调优和诊断问题的时间。基于这个考虑，我个人建议将 swappniess 配置成一个接近 0 但不为 0 的值，比如 1。 提交时间：即 Flush 落盘时间。向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的“脏”数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。一般情况下我们会认为这个时间太频繁了，可以适当地增加提交间隔来降低物理磁盘的写操作。当然你可能会有这样的疑问：如果在页缓存中的数据在写入到磁盘前机器宕机了，那岂不是数据就丢失了。的确，这种情况数据确实就丢失了，但鉴于 Kafka 在软件层面已经提供了多副本的冗余机制，因此这里稍微拉大提交间隔去换取性能还是一个合理的做法。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:2:2","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"生产端和消费端 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:3:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"压缩 Producer 端压缩、Broker 端保持、Consumer 端解压缩。 在吞吐量方面：LZ4 \u003e Snappy \u003e zstd 和 GZIP；而在压缩比方面，zstd \u003e LZ4 \u003e GZIP \u003e Snappy。因此，推荐LZ4。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:3:1","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"拦截器 在应用程序不修改的情况下，动态的实现一组可插拔的事件处理逻辑链。它能在主业务操作的前后多个时间点上插入对应的拦截逻辑。可用于客户端监控、添加消息头、审计、端到端系统性能监测等。 Properties props = new Properties(); List\u003cString\u003e interceptors = new ArrayList\u003c\u003e(); interceptors.add(\"com.yourcompany.kafkaproject.interceptors.AddTimestampInterceptor\"); // 拦截器1 interceptors.add(\"com.yourcompany.kafkaproject.interceptors.UpdateCounterInterceptor\"); // 拦截器2 props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); 生产者拦截器 实现 org.apache.kafka.clients.producer.ProducerInterceptor 接口： onSend：该方法会在消息发送之前被调用 onAcknowledgement：该方法会在消息成功提交或发送失败之后被调用，早于 callback 的调用 消费者拦截器 实现 org.apache.kafka.clients.consumer.ConsumerInterceptor 接口： onConsume：该方法在消息返回给 Consumer 程序之前调用 onCommit：Consumer 在提交位移之后调用该方法 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:3:2","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"生产者如何管理TCP连接 13 | Java生产者是如何管理TCP连接的？ (geekbang.org) KafkaProducer 实例创建时启动 Sender 线程，从而创建与 bootstrap.servers 中所有 Broker 的 TCP 连接。 KafkaProducer 实例首次更新元数据信息之后，还会再次创建与集群中所有 Broker 的 TCP 连接。 如果 Producer 端发送消息到某台 Broker 时发现没有与该 Broker 的 TCP 连接，那么也会立即创建连接。 如果设置 Producer 端 connections.max.idle.ms 参数（默认9分钟）大于 0，则步骤 1 中创建的 TCP 连接会被自动关闭；如果设置该参数 =-1，那么步骤 1 中创建的 TCP 连接将无法被关闭，从而成为“僵尸”连接（TCP 连接是在 Broker 端被关闭的，但其实这个 TCP 连接的发起方是客户端，因此在 TCP 看来，这属于被动关闭的场景，即 passive close。被动关闭的后果就是会产生大量的 CLOSE_WAIT 连接，因此 Producer 端或 Client 端没有机会显式地观测到此连接已被中断）。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:3:3","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"消费者组 重平衡 重平衡很慢，效率低，应尽量避免。因为参数或逻辑不合理（poll之后处理时间太长）而导致的组成员意外离组或退出： session.timeout.ms 默认10s heartbeat.interval.ms 默认3s max.poll.interval.ms 默认300s GC 参数。程序频发 Full GC 引发的非预期 Rebalance 成员被动退出后，再次去提交offset时，会报错： org.apapache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=xx, groupId=xx] Offset commit failed on partition xx-1 at offset xx: The coordinator is not aware of this member. ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:3:4","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"多线程消费 Java Consumer 是双线程的设计。一个线程是用户主线程，负责获取消息；另一个线程是心跳线程，负责向 Kafka 汇报消费者存活情况。将心跳单独放入专属的线程，能够有效地规避因消息处理速度慢而被视为下线的“假死”情况。 多线程方案： 消费者程序启动多个线程，每个线程维护专属的 KafkaConsumer 实例，负责完整的消息获取、消息处理流程。【实例是各自独立的】 消费者程序使用单或多线程获取消息，同时创建多个消费线程执行消息处理逻辑。拉取消息的consumer线程与实际干活的worker是1:n的关系。 优势：伸缩性好。worker线程池和拉消息的线程独立，可以分别扩展 劣势：无法保证顺序。eg：一个consumer实例拉取一批消息，交给10个worker线程处理，这批消息的处理顺序就打乱了。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:3:5","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"消费者是如何管理TCP连接 何时创建 TCP 连接 和生产者不同的是，构建 KafkaConsumer 实例时是不会创建任何 TCP 连接的，也就是说，当你执行完 new KafkaConsumer(properties) 语句后，你会发现，没有 Socket 连接被创建出来。这一点和 Java 生产者是有区别的，主要原因就是生产者入口类 KafkaProducer 在构建实例的时候，会在后台默默地启动一个 Sender 线程，这个 Sender 线程负责 Socket 连接的创建。 消费者的TCP 连接是在调用 KafkaConsumer.poll 方法时被创建的。再细粒度地说，在 poll 方法内部有 3 个时机可以创建 TCP 连接： 发起 FindCoordinator 请求时，希望 Kafka 集群告诉它哪个 Broker 是管理它的协调者。消费者应该向哪个 Broker 发送这类请求呢？理论上任何一个 Broker 都能回答这个问题，也就是说消费者可以发送 FindCoordinator 请求给集群中的任意服务器。在这个问题上，社区做了一点点优化：消费者程序会向集群中当前负载最小的那台 Broker 发送请求。负载是如何评估的呢？其实很简单，就是看消费者连接的所有 Broker 中，谁的待发送请求最少。 连接协调者时 消费数据时 何时关闭 TCP 连接 和生产者类似，消费者关闭 Socket 也分为主动关闭和 Kafka 自动关闭。 主动关闭：KafkaConsumer.close() 方法，或 Kill -2 、Kill -9 自动关闭：由消费者端参数 connection.max.idle.ms 控制，默认 9 分钟 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:3:6","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"消费者组重平衡 消费者端参数 heartbeat.interval.ms 的真实用途：从字面上看，它就是设置了心跳的间隔时间，但这个参数的真正作用是控制重平衡通知的频率。如果你想要消费者实例更迅速地得到通知，那么就可以给这个参数设置一个非常小的值，这样消费者就能更快地感知到重平衡已经开启了。 消费者组的状态 含义 Empty 组内没有任何成员，但消费者组可能存在已提交的位移数据，而且这些位移尚未过期。 Dead 同样是组内没有任何成员，但组的元数据信息已经在协调者端被移除。 协调者组件保存着当前向它注册过的所有组信息，所谓的元数据信息就类似于这个注册信息。 PreparingRebalance 消费者组准备开启重平衡，此时所有成员都要重新请求加入消费者组。 CompleingRebalance 消费者组下所有成员已经加入，各个成员正在等待分配方案。 该状态在老一点的版本中被称为AwaitingSync,它和CompletingRebalance是等价的。 Stable 消费者组的稳定状态。该状态表明重平衡已经完成，组内各成员能够正常消费数据了。 一个消费者组最开始是 Empty 状态，当重平衡过程开启后，它会被置于 PreparingRebalance 状态等待成员加入，之后变更到 CompletingRebalance 状态等待分配方案，最后流转到 Stable 状态完成重平衡。 当有新成员加入或已有成员退出时，消费者组的状态从 Stable 直接跳到 PreparingRebalance 状态，此时，所有现存成员就必须重新申请加入组。当所有成员都退出组后，消费者组状态变更为 Empty。Kafka 定期自动删除过期位移的条件就是，组要处于 Empty 状态。因此，如果你的消费者组停掉了很长时间（超过 7 天），那么 Kafka 很可能就把该组的位移数据删除了。我相信，你在 Kafka 的日志中一定经常看到下面这个输出： Removed xxx expired offsets in xxx milliseconds. 这就是 Kafka 在尝试定期删除过期位移。现在你知道了，只有 Empty 状态下的组，才会执行过期位移删除的操作。 重平衡分为两个步骤：分别是加入组和等待领导者消费者（Leader Consumer）分配方案。这两个步骤分别对应两类特定的请求：JoinGroup 请求和 SyncGroup 请求。第一个发送 JoinGroup 请求的成员自动成为领导者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:3:7","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"Broker端 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:4:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"控制器 它的主要作用是在 ZooKeeper 的帮助下管理和协调整个 Kafka 集群。集群中任意一台 Broker 都能充当控制器的角色，但是，在运行过程中，只能有一个 Broker 成为控制器。第一个在 ZooKeeper 成功创建 /controller 节点的 Broker 会被指定为控制器。 控制器是做什么的？ 主题管理（创建、删除、增加分区） 分区重分配 Preferred 领导者选举：为了避免部分 Broker 负载过重而提供的一种换 Leader 的方案 集群成员管理（新增 Broker、Broker 主动关闭、Broker 宕机），依赖 ZooKeeper Watch、临时节点 数据服务。控制器上保存了最全的集群元数据信息，其他所有 Broker 会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据。这些数据其实在 ZooKeeper 中也保存了一份。每当控制器初始化时，它都会从 ZooKeeper 上读取对应的元数据并填充到自己的缓存中。 控制器内部设计原理 多线程访问共享可变数据是维持线程安全最大的难题。为了保护数据安全性，控制器不得不在代码中大量使用 ReentrantLock 同步机制。社区于 0.11 版本重构了控制器的底层设计，最大的改进就是，把多线程的方案改成了单线程加事件队列的方案。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:4:1","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"高水位和Leader Epoch Kafka 的水位不是时间戳，更与时间无关。它是和位置信息绑定的，具体来说，它是用消息位移来表征的。另外，Kafka 源码使用的表述是高水位，因此，今天我也会统一使用“高水位”或它的缩写 HW 来进行讨论。值得注意的是，Kafka 中也有低水位（Low Watermark），它是与 Kafka 删除消息相关联的概念，与今天我们要讨论的内容没有太多联系。 高水位的作用 定义消息可见性，即用来标识分区下的哪些消息是可以被消费者消费的。 帮助 Kafka 完成副本同步。 这里的提交不是指事务的commit，已经复制到ISR的消息，称为已提交的消息。参考下图： 来源：《Kafka权威指南》image.png \"\r来源：《Kafka权威指南》\r 在分区高水位以下的消息被认为是已提交消息，反之就是未提交消息。消费者只能消费已提交消息，即图中位移小于 8 的所有消息。注意，这里我们不讨论 Kafka 事务，因为事务机制会影响消费者所能看到的消息的范围，它不只是简单依赖高水位来判断。它依靠一个名为 LSO（Last Stable Offset）的位移值来判断事务型消费者的可见性。 另外，需要关注的是，位移值等于高水位的消息也属于未提交消息。也就是说，高水位上的消息是不能被消费者消费的。 图中还有一个日志末端位移的概念，即 Log End Offset，简写是 LEO。它表示副本写入下一条消息的位移值。注意，数字 15 所在的方框是虚线，这就说明，这个副本当前只有 15 条消息，位移值是从 0 到 14，下一条新消息的位移是 15。显然，介于高水位和 LEO 之间的消息就属于未提交消息。这也从侧面告诉了我们一个重要的事实，那就是：同一个副本对象，其高水位值不会大于 LEO 值。 Kafka 使用 Leader 副本的高水位来定义所在分区的高水位。 Leader Epoch 高水位在界定 Kafka 消息对外可见性以及实现副本机制等方面起到了非常重要的作用，但其设计上的缺陷给 Kafka 留下了很多数据丢失或数据不一致的潜在风险。为此，社区引入了 Leader Epoch 机制，尝试规避掉这类风险。 Leader Epoch 大致可以认为是 Leader 版本。它由两部分数据组成： Epoch。一个单调增加的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的 Leader 被认为是过期 Leader，不能再行使 Leader 权力。 起始位移（Start Offset）。Leader 副本在该 Epoch 值上写入的首条消息的位移。 高水位与事务消息的关系 in read_committed mode, consumer.poll() will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction. In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed. As a result, read_committed consumers will not be able to read up to the high watermark when there are in flight transactions. Further, when in read_committed the seekToEnd method will return the LSO。 引申：KafkaConsumer#endOffsets(Collection partitions)方法，注释如下： Get the end offsets for the given partitions. In the default read_uncommitted isolation level, the end offset is the high watermark (that is, the offset of the last successfully replicated message plus one). For read_committed consumers, the end offset is the last stable offset (LSO), which is the minimum of the high watermark and the smallest offset of any open transaction. Finally, if the partition has never been written to, the end offset is 0. This method does not change the current consumer position of the partitions. ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:4:2","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"副本机制 提供数据冗余 提供高伸缩性。增加机器-\u003e提升读性能-\u003e提高读操作吞吐量 改善数据局部性 Leader Replica 和 Follower Replica，Follower 不对外提供服务，原因： 方便实现“Read-your-writes”：当你使用生产者 API 向 Kafka 成功写入消息后，马上使用消费者 API 去读取刚才生产的消息。发微博时，你发完一条微博，肯定是希望能立即看到的，这就是典型的 Read-your-writes 场景。 方便实现单调读（Monotonic Reads）：对于一个消费者用户而言，在多次消费消息时，它不会看到某条消息一会儿存在一会儿不存在。 In-sync Replicas（ISR） ISR 不只是追随者副本集合，它必然包括 Leader 副本。 Broker 端参数 replica.lag.time.max.ms：指定 Follower 副本能够落后 Leader 副本的最长时间间隔，默认值是 10 秒。 日志截断 类似数据库的回滚，删除无效的消息。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:4:3","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"请求是怎么被处理的 所有的请求都是通过 TCP 网络以 Socket 的方式进行通讯的。 处理请求的 2 种常见方案 （1）顺序处理请求。吞吐量太差 while (true) { Request request = accept(connection); handle(request); } （2）每个请求使用单独线程处理，异步。每个请求都创建线程，开销大 while (true) { Request = request = accept(connection); Thread thread = new Thread(() -\u003e { handle(request);}); thread.start(); } Kafka 使用 Reactor 模式 Reactor 模式是事件驱动架构的一种实现方式，用于处理多个客户端并发向服务器端发送请求的场景。Reactor 有个请求分发线程 Dispatcher，也就是图中的 Acceptor。Kafka 提供了 Broker 端参数 num.network.threads，用于调整该网络线程池的线程数。其默认值是 3，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求。 客户端发来的请求会被 Broker 端的 Acceptor 线程分发到任意一个网络线程中，网络线程拿到请求后，它不是自己处理，而是将请求放入到一个共享请求队列中。Broker 端还有个 IO 线程池，负责从该队列中取出请求，执行真正的处理。Broker 端参数 num.io.threads 控制了这个线程池中的线程数。目前该参数默认值是 8，表示每台 Broker 启动后自动创建 8 个 IO 线程处理请求。 图中有一个叫 Purgatory 的组件，这是 Kafka 中著名的“炼狱”组件。它是用来缓存延时请求（Delayed Request）的。所谓延时请求，就是那些一时未满足条件不能立刻处理的请求。比如设置了 acks=all 的 PRODUCE 请求，一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回，此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:4:4","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"日志 Kafka 源码使用ConcurrentSkipListMap类型来保存日志段对象。好处有两个：线程安全；支持 Key 的排序。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:4:5","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"消息语义 至多一次 至少一次 有且仅有一次 Kafka默认保证至少一次。也可以保证最多一次，只需让 Producer 禁止重试。 无消息丢失配置 一句话概括，Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证。 一句话概括，Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证。有限度：假如你的消息保存在 N 个 Kafka Broker 上，那么这个前提条件就是这 N 个 Broker 中至少有 1 个存活。只要这个条件成立，Kafka 就能保证你的这条消息永远不会丢失。 Producer端： 使用 producer.send(msg, callback)，而非producer.send(msg) acks = all retries \u003e 0，网络抖动时重试 Broker端： unclean.leader.election.enable = false，不允许落后太多的follower成为leader replication.factor \u003e= 3，多副本 min.insync.replicas \u003e 1，消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。默认值 1。 确保 replication.factor \u003e min.insync.replicas，如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。 Consumer端： enable.auto.commit = false，手动提交位移 幂等 事务 如何在业务中去重 设置特别的业务字段，用于标识消息的id，再次遇到相同id则直接确认消息； 将业务逻辑设计为幂等，即使发生重复消费，也能保证一致性； 如何保证分区级消息有序 max.in.flight.requests.per.connection=1 Kafka 使用 Compact 策略来删除位移主题（__consumer_offsets）中的过期消息，避免该主题无限期膨胀（使用自送提交时，在闲时状态，消费者会提交很多相同offset的进度）。后台线程 Log Cleaner 会定期地巡检待 Compact 的主题，删除过期消息。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:5:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"运维监控 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"自带脚本 这些命令行脚本很多都是通过连接 ZooKeeper 来提供服务的。kafka-topics 脚本连接 ZooKeeper 时，不会考虑 Kafka 设置的用户认证机制。 测试生产者/消费者性能 kafka-xx-perf-test 查看主题消息总数 Kafka 自带的命令没有提供这样的功能。可以使用 Kafka 提供的工具类 GetOffsetShell 来计算给定主题特定分区当前的最早位移和最新位移，将两者的差值累加起来，就能得到该主题当前总的消息数。对于本例来说，test-topic 总的消息数为 5500000 + 5500000，等于 1100 万条： $ bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list kafka-host:port --time -2 --topic test-topic test-topic:0:0 test-topic:1:0 $ bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list kafka-host:port --time -1 --topic test-topic test-topic:0:5500000 test-topic:1:5500000 查看消息文件数据 kafka-dump-log： $ bin/kafka-dump-log.sh --files ../data_dir/kafka_1/test-topic-1/00000000000000000000.log --deep-iteration --print-data-log ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:1","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"AdminClient 服务器端也有一个 AdminClient，包路径是 kafka.admin。这是之前的老运维工具类，提供的功能也比较有限，社区已经不再推荐使用它了。所以，我们最好统一使用客户端的 AdminClient。 工作原理 AdminClient 是一个双线程的设计：前端主线程和后端 I/O 线程。前端线程负责将用户要执行的操作转换成对应的请求，然后再将请求发送到后端 I/O 线程的队列中；而后端 I/O 线程从队列中读取相应的请求，然后发送到对应的 Broker 节点上，之后把执行结果保存起来，以便等待前端线程的获取。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:2","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"认证授权 认证 截止到当前最新的 2.3 版本，Kafka 支持基于 SSL 和基于 SASL 的安全认证机制。基于 SSL 的认证主要是指 Broker 和客户端的双路认证（2-way authentication）。通常来说，SSL 加密（Encryption）已经启用了单向认证，即客户端认证 Broker 的证书（Certificate）。如果要做 SSL 认证，那么我们要启用双路认证，也就是说 Broker 也要认证客户端的证书。 Kafka 还支持通过 SASL 做客户端认证。SASL 是提供认证和数据安全服务的框架。Kafka 支持的 SASL 机制有 5 种： GSSAPI：也就是 Kerberos 使用的安全接口，是在 0.9 版本中被引入的。适用于本身已经做了 Kerberos 认证的场景，这样的话，SASL/GSSAPI 可以实现无缝集成 PLAIN：是使用简单的用户名 / 密码认证的机制，在 0.10 版本中被引入。配置和运维成本小。不能动态地增减认证用户：所有认证用户信息全部保存在静态文件中，所以只能重启 Broker，才能重新加载变更后的静态文件 SCRAM：主要用于解决 PLAIN 机制安全问题的新机制，是在 0.10.2 版本中被引入的。如果你打算使用 SASL/PLAIN，不妨改用 SASL/SCRAM OAUTHBEARER：是基于 OAuth 2 认证框架的新机制，在 2.0 版本中被引进。 Delegation Token：补充现有 SASL 机制的轻量级认证机制，是在 1.1.0 版本被引入的。 授权 34 | 云环境下的授权该怎么做？ (geekbang.org) 所谓授权，一般是指对与信息安全或计算机安全相关的资源授予访问权限，特别是存取控制。具体到权限模型，常见的有四种。 ACL：Access-Control List，访问控制列表。 RBAC：Role-Based Access Control，基于角色的权限控制。 ABAC：Attribute-Based Access Control，基于属性的权限控制。 PBAC：Policy-Based Access Control，基于策略的权限控制。 在典型的互联网场景中，前两种模型应用得多，后面这两种则比较少用。ACL 模型很简单，它表征的是用户与权限的直接映射关系；而 RBAC 模型则加入了角色的概念，支持对用户进行分组。 Kafka 用的是 ACL 模型。简单来说，这种模型就是规定了什么用户对什么资源有什么样的访问权限。我们可以借用官网的一句话来统一表示这种模型：“Principal P is [Allowed/Denied] Operation O From Host H On Resource R.” 这句话中出现了很多个主体： Principal：表示访问 Kafka 集群的用户。 Operation：表示一个具体的访问类型，如读写消息或创建主题等。 Host：表示连接 Kafka 集群的客户端应用程序 IP 地址。Host 支持星号占位符，表示所有 IP 地址。 Resource：表示 Kafka 资源类型。如果以最新的 2.3 版本为例，Resource 共有 5 种，分别是 TOPIC、CLUSTER、GROUP、TRANSACTIONALID 和 DELEGATION TOKEN。 当前，Kafka 提供了一个可插拔的授权实现机制。该机制会将你配置的所有 ACL 项保存在 ZooKeeper 下的 /kafka-acl 节点中。你可以通过 Kafka 自带的 kafka-acls 脚本动态地对 ACL 项进行增删改查，并让它立即生效。 开启 ACL 的方法特别简单，只需要在 Broker 端的配置文件 server.properties 文件中配置下面这个参数值： authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer Kafka 的授权机制可以不配置认证机制单独使用，但是只能为IP地址设置权限。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:3","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"跨集群备份MirrorMaker MirrorMaker本质是从源集群消费数据，然后用生产者发送到目标集群。 它提供的功能很有限，运维成本高，比如主题的管理就非常不便捷，同时也很难将其管道化。基于这些原因，业界很多公司选择自己开发跨集群镜像工具： Uber 的 uReplicator LinkedIn 开发的 Brooklin Mirror Maker Confluent 公司研发的 Replicator。最强，收费 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:4","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"监控 主机监控 JVM监控：监控 Broker GC 日志，即以 kafkaServer-gc.log 开头的文件。注意不要出现 Full GC 的字样。一旦发现 Broker 进程频繁 Full GC，可以开启 G1 的 -XX:+PrintAdaptiveSizePolicy 开关，让 JVM 告诉你到底是谁引发了 Full GC。 集群监控 监控框架/工具 JMXTool 工具：kafka-run-class.sh kafka.tools.JmxTool，自带的一个工，能够实时查看 Kafka JMX 指标 Kafka Manager：久不更新，活跃的代码维护者只有三四个人，无法追上 Apache Kafka 版本的更迭速度 Burrow：LinkedIn 开源的一个专门监控消费者进度的框架，用 Go 写的，没有 UI 界面，只是开放了一些 HTTP Endpoint，久不更新 JMXTrans + InfluxDB + Grafana Confluent Control Center：最强大的，企业级，收费 Kafka Eagle：国人维护，积极演进。除了常规的监控功能之外，还开放了告警功能（Alert），非常值得一试 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:5","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"调优 优化漏斗：层级越靠上，其调优的效果越明显 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:6","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"动态参数配置 无需重启Broker就能立即生效的参数。 动态调整Broker端各种线程池大小，实时应对突发流量 动态调整Broker端Compact操作性能 动态调整Broker端连接信息或安全配置信息 动态更新SSL Keystore有效期 实时变更JMX指标收集器 log.retention.ms num.io.threads和num.network.threads num.replica.fetchers，不超过CPU核数 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:7","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"node id: -1是什么意思 如下的日志里面： [2019-05-27 10:00:54,142] DEBUG [Consumer clientId=consumer-1, groupId=test] Initiating connection to node localhost:9092 (id: -1 rack: null) using address localhost/127.0.0.1 (org.apache.kafka.clients.NetworkClient:944) [2019-05-27 10:00:54,188] DEBUG [Consumer clientId=consumer-1, groupId=test] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name=‘t4’)], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node localhost:9092 (id: -1 rack: null) (org.apache.kafka.clients.NetworkClient:1097) [2019-05-27 10:00:54,188] TRACE [Consumer clientId=consumer-1, groupId=test] Sending FIND_COORDINATOR {key=test,key_type=0} with correlation id 0 to node -1 (org.apache.kafka.clients.NetworkClient:496) [2019-05-27 10:00:54,203] TRACE [Consumer clientId=consumer-1, groupId=test] Completed receive from node -1 for FIND_COORDINATOR with correlation id 0, received {throttle_time_ms=0,error_code=0,error_message=null, 日志的第一行是消费者程序创建的第一个 TCP 连接，就像我们前面说的，这个 Socket 用于发送 FindCoordinator 请求。由于这是消费者程序创建的第一个连接，此时消费者对于要连接的 Kafka 集群一无所知，因此它连接的 Broker 节点的 ID 是 -1，表示消费者根本不知道要连接的 Kafka Broker 的任何信息。 那么 2147483645 这个 id 是怎么来的呢？它是由 Integer.MAX_VALUE 减去协调者所在 Broker 的真实 ID 计算得来的。看第四行标为橙色的内容，我们可以知道协调者 ID 是 2，因此这个 Socket 连接的节点 ID 就是 Integer.MAX_VALUE 减去 2，即 2147483647 减去 2，也就是 2147483645。这种节点 ID 的标记方式是 Kafka 社区特意为之的结果，目的就是要让组协调请求和真正的数据获取请求使用不同的 Socket 连接。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:8","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"CommitFailedException 这个异常是由于消费者在提交进度时，发现自己已经被开除了，它消费的分区分给别人了。the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms。解决方法： 减少每一批拉取的消息数（max.poll.records） 增加poll间隔时间上限（max.poll.interval.ms） 加快处理速度——多线程消费方案。poll的一批消息交给多个线程处理，使用并发提升吞吐。 前提：无顺序要求。 难点：进度的提交。poll消息的主线程应该等待所有子线程处理完毕再提交进度。如果某个线程失败了，应用重启后，其他线程再次处理重复消息，需手动去重。 另一个可能的原因，如果standalone组与某个消费group有相同的groupId，也会造成该异常。 groupId和clientId都相同的两个客户端，会造成什么后果？ ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:9","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"Kafka 能手动删除消息吗 提供了留存策略，能够自动删除过期消息，不需用户手动删除消息。当然，它支持手动删除消息： 对于设置了 Key 且参数 cleanup.policy=compact 的主题而言，我们可以构造一条key=null的消息发送给 Broker，依靠 Log Cleaner 组件提供的功能删除掉该 Key 的消息。 对于普通主题而言，我们可以使用 kafka-delete-records 命令，或编写程序调用 Admin.deleteRecords 方法来删除消息。这两种方法殊途同归，底层都是调用 Admin 的 deleteRecords 方法，通过将分区 Log Start Offset 值抬高的方式间接删除消息。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:6:10","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"流处理 Kafka Connect + Kafka Core + Kafka Streams ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:7:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"其他 社区最近正在花大力气去优化消费者组机制，力求改善因 Rebalance 导致的各种场景，但其实，其他框架开发者反而是不用 Group 机制的。他们宁愿自己开发一套机制来维护分区分配的映射。这些都说明 Kafka 中的消费者组还是有很大的提升空间的。 确实，我们的框架也是自己维护分区映射。 ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:8:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":["kafka"],"content":"Reference 极客时间 - Kafka 核心技术与实战 《Kafka权威指南》 https://kafka.apache.org/documentation ","date":"2022-03-25","objectID":"/2022-03-25-geektime-core-kafka-in-action-notes/:9:0","tags":["kafka"],"title":"《Kafka 核心技术与实战》学习笔记","uri":"/2022-03-25-geektime-core-kafka-in-action-notes/"},{"categories":null,"content":"问题描述 有一个配置文件 config： KAFKA_HOME=/home/wy/dev/kafka_2.13-2.6.0 BOOTSTRAP_SERVER=127.0.0.1:9092 另有一个使用该配置文件的脚本 list-topics.sh： #!/bin/bash . config \"$KAFKA_HOME\"/bin/kafka-topics.sh --bootstrap-server \"$BOOTSTRAP_SERVER\" --list 我使用虚拟机Ubuntu挂载Windows分区，执行在Windows环境下编写的脚本，提示： wy@ship:/mnt/hgfs/D/projects/kafka-mate/scripts$ ./list-topics.sh /bin/kafka-topics.sh: No such file or directory13-2.6.0 ","date":"2022-03-13","objectID":"/2022-03-13-linebreak-in-windows-and-linux/:1:0","tags":["换行符"],"title":"Windows和Linux的不同换行符导致shell脚本执行异常","uri":"/2022-03-13-linebreak-in-windows-and-linux/"},{"categories":null,"content":"解决 找了好久的原因，最终注意到config文件的换行符编码为CRLF。 三种换行符（line separator）： Windows：CRLF(\\r\\n) Unix and macOSLinux：LF(\\n) CLassic Mac OS：CR(\\r)，少见 改为LF后，脚本运行正常。我用的IDEA，在右下角更改： ","date":"2022-03-13","objectID":"/2022-03-13-linebreak-in-windows-and-linux/:2:0","tags":["换行符"],"title":"Windows和Linux的不同换行符导致shell脚本执行异常","uri":"/2022-03-13-linebreak-in-windows-and-linux/"},{"categories":null,"content":"vim状态栏的[noeol] [dos]是什么意思？ 当config的line separator问CRLF时，vim打开Windows分区上的文件时，状态栏显示[noeol][dos]： 当config的line separator问LF时，vim打开Windows分区上的文件时，状态栏显示[noeol]： noeol：no end of line。 如果是Linux分区上的文件，且line separator为LF时，状态栏不会额外显示。 以下内容来自：Vim 编辑器底端 [noeol], [dos] 的含义 - Trekshot - 博客园 (cnblogs.com) 有时使用 Vim 打开一个文件会在窗口底部发现 [noeol], [dos] 提示信息： \"hello-dos.txt\" [noeol][dos] 2L, 10C 1,1 All 这两个符号有何含义？ 直观上理解，‘noeol’ 就是 ‘no end-of-line’, 即“没有行末结束符”， Linux 下的文本编辑器（如 Vim）会在每一行 （包括最后一行）末尾添加一个换行符。比如我们在 Debian 下新建一个名为 ‘hello-unix.txt’ 的文本文件，内容如下： Hello Unix 那么，使用cat -A hello-unix.txt命令可以看到这些换行符： ts@TS:~/www/document$ cat -A hello-unix.txt Hello$ Unix$ ts@TS:~/www/document$ 从中可以清楚地看到每行末尾的 ‘$’ 字符，这就是 Linux 下的“行末结束符”。 下面我们再在 Windows 下创建一个名为 ‘hello-dos.txt’ 记事本文件，内容如下： Hello DOS 在 Debian 下查看此文件的换行符信息： ts@TS:~/www/document$ cat -A hello-dos.txt Hello^M$ DOSts@TS:~/www/document$ 同样是两行，每行一个单词，Windows 和 Linux 下的换行符有两个明显不同： Windows 下的换行符比 Linux 下的多了个 ^M； 最后一行行末没有换行符； 这两个不同之处也正是 [dos], [noeol] 两个 Flag 信息出现的原因。 Windows 下文本文件每行的换行符为“回车+换行”(CRLF,^M$), 而 Linux 下则仅为 “换行” (LF, $). Vim 发现文本中含有 ^M$换行字符判定为 Windows 下创建的 文件，用 [dos] Flag 提示；Vim 没有在最后一行发现换行符，判定此文件不是在 Linux 下创建/编辑，用 [noeol] Flag 提示用户。 ","date":"2022-03-13","objectID":"/2022-03-13-linebreak-in-windows-and-linux/:3:0","tags":["换行符"],"title":"Windows和Linux的不同换行符导致shell脚本执行异常","uri":"/2022-03-13-linebreak-in-windows-and-linux/"},{"categories":null,"content":"如何消除 [noeol] Flag？ 只需在 Debian 下将该文件重新保存即可，还是上面的 hello-dos.txt 文件，打开它， 不做任何修改直接 :wq保存退出，再查看换行符： ts@TS:~/www/document$ cat -A hello-dos.txt Hello^M$ DOS^M$ ts@TS:~/www/document$ 换行符已经追加上去，这里要注意的是追加的是 Windows 下的换行符（回车+换行） ^M$, 而不是 Linux 下的换行符（换行）$, 因为 Vim 已经发现此文件是在 Windows 下创建的（[dos] Flag），尽管是在 Linux 下编辑，Vim 也会按照文件创建时所在的操作系统下的换行规则添加换行符。 ","date":"2022-03-13","objectID":"/2022-03-13-linebreak-in-windows-and-linux/:3:1","tags":["换行符"],"title":"Windows和Linux的不同换行符导致shell脚本执行异常","uri":"/2022-03-13-linebreak-in-windows-and-linux/"},{"categories":null,"content":"如何消除 [dos] Flag？ 有两种简单的方法： Linux 下提供有两个命令用来进行 Windows 和 Unix 文件的转化：dos2unix 和 unix2dos; 在 Debian 下使用 touch template.txt 创建一个模板，在 Windows 下创建的任 何文本文件都以此模板为基础； ","date":"2022-03-13","objectID":"/2022-03-13-linebreak-in-windows-and-linux/:3:2","tags":["换行符"],"title":"Windows和Linux的不同换行符导致shell脚本执行异常","uri":"/2022-03-13-linebreak-in-windows-and-linux/"},{"categories":null,"content":"Reference Vim 编辑器底端 [noeol], [dos] 的含义 - Trekshot - 博客园 (cnblogs.com) ","date":"2022-03-13","objectID":"/2022-03-13-linebreak-in-windows-and-linux/:4:0","tags":["换行符"],"title":"Windows和Linux的不同换行符导致shell脚本执行异常","uri":"/2022-03-13-linebreak-in-windows-and-linux/"},{"categories":["kafka"],"content":"问题描述 删除topic时导致集群崩溃，报错： ERROR Shutdown broker because all log dirs in D:\\tmp\\kafka-logs have failed. 测试了kafka_2.11-1.1.0、kafka_2.13-2.5.0、kafka_2.13-2.6.2、kafka_2.13-2.7.0四个版本，都有这个问题。 搜索了网络，发现这个bug很早之前就提出了，但至今没解决。 https://issues.apache.org/jira/browse/KAFKA-10419 https://issues.apache.org/jira/browse/KAFKA-9458 https://github.com/apache/kafka/pull/6329 https://stackoverflow.com/questions/50755827/accessdeniedexception-when-deleting-a-topic-on-windows-kafka Linux上无此问题。 使用如下命令即可复现： D:\\kafka_2.13-2.6.2\u003ebin\\windows\\kafka-topics.bat --bootstrap-server 127.0.0.1:9092 --create --topic testwy Created topic testwy. D:\\kafka_2.13-2.6.2\u003ebin\\windows\\kafka-topics.bat --bootstrap-server 127.0.0.1:9092 --list testwy D:\\kafka_2.13-2.6.2\u003ebin\\windows\\kafka-topics.bat --bootstrap-server 127.0.0.1:9092 --delete --topic testwy 此时集群崩溃，zookeeper日志： [2021-07-05 16:32:52,965] WARN Exception causing close of session 0x1003d29550c0000: 远程主机强迫关闭了一个现有的连接。 (org.apache.zookeeper.server.NIOServerCnxn) [2021-07-05 16:33:11,170] INFO Expiring session 0x1003d29550c0000, timeout of 18000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer) 但zk仍然在运行。 kafka server日志： [2021-07-05 16:32:52,574] INFO [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: testwy-0. (kafka.coordinator.group.GroupCoordinator) [2021-07-05 16:32:52,599] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(testwy-0) (kafka.server.ReplicaFetcherManager) [2021-07-05 16:32:52,600] INFO [ReplicaAlterLogDirsManager on broker 0] Removedfetcher for partitions Set(testwy-0) (kafka.server.ReplicaAlterLogDirsManager) [2021-07-05 16:32:52,607] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(testwy-0) (kafka.server.ReplicaFetcherManager) [2021-07-05 16:32:52,608] INFO [ReplicaAlterLogDirsManager on broker 0] Removedfetcher for partitions Set(testwy-0) (kafka.server.ReplicaAlterLogDirsManager) [2021-07-05 16:32:52,615] ERROR Error while renaming dir for testwy-0 in log dir D:\\tmp\\kafka-logs (kafka.server.LogDirFailureChannel) java.nio.file.AccessDeniedException: D:\\tmp\\kafka-logs\\testwy-0 -\u003e D:\\tmp\\kafka-logs\\testwy-0.a61ed5f8a99e4df58e8bf86c6c5e537c-delete at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89) at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103) at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395) at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:288) at java.base/java.nio.file.Files.move(Files.java:1421) at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:917) at kafka.log.Log.$anonfun$renameDir$2(Log.scala:1012) at kafka.log.Log.renameDir(Log.scala:2387) at kafka.log.LogManager.asyncDelete(LogManager.scala:973) at kafka.log.LogManager.$anonfun$asyncDelete$3(LogManager.scala:1008) at kafka.log.LogManager.$anonfun$asyncDelete$2(LogManager.scala:1006) at kafka.log.LogManager.$anonfun$asyncDelete$2$adapted(LogManager.scala:1004) at scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:435) at scala.collection.mutable.HashSet.foreach(HashSet.scala:361) at kafka.log.LogManager.asyncDelete(LogManager.scala:1004) at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:481) at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:271) at kafka.server.KafkaApis.handle(KafkaApis.scala:142) at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:74) at java.base/java.lang.Thread.run(Thread.java:834) Suppressed: java.nio.file.AccessDeniedException: D:\\tmp\\kafka-logs\\testwy-0 -\u003e D:\\tmp\\kafka-logs\\testwy-0.a61ed5f8a99e4df58e8bf86c6c5e537c-delete at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89) at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103) at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309) at java.base/sun.nio.fs.WindowsFileSystemProvider.move(","date":"2022-03-12","objectID":"/2022-03-12-can-not-delete-topics-when-kafka-running-on-windows/:1:0","tags":["kafka","kafka topic"],"title":"kafka在Windows上无法动态删除topic","uri":"/2022-03-12-can-not-delete-topics-when-kafka-running-on-windows/"},{"categories":["kafka"],"content":"解决办法 直接重启kafka-sever依然会报错。需先删除tmp目录下的全部log文件，然后再重启。 ","date":"2022-03-12","objectID":"/2022-03-12-can-not-delete-topics-when-kafka-running-on-windows/:2:0","tags":["kafka","kafka topic"],"title":"kafka在Windows上无法动态删除topic","uri":"/2022-03-12-can-not-delete-topics-when-kafka-running-on-windows/"},{"categories":null,"content":" 官方地址： shadowsocks/shadowsocks-libev: Bug-fix-only libev port of shadowsocks. Future development moved to shadowsocks-rust (github.com) 旧地址，不再更新： shadowsocks/shadowsocks at master (github.com) clowwindy/shadowsocks-libev at master (github.com) 本文基于Ubuntu。 ","date":"2022-03-09","objectID":"/2022-03-09-shadowsocks/:0:0","tags":["shadowsocks","vpn"],"title":"shadowsocks 服务端安装指南","uri":"/2022-03-09-shadowsocks/"},{"categories":null,"content":"install $ sudo apt install shadowsocks-libev ... Created symlink /etc/systemd/system/multi-user.target.wants/shadowsocks-libev.service → /lib/systemd/system/shadowsocks-libev.service. ","date":"2022-03-09","objectID":"/2022-03-09-shadowsocks/:1:0","tags":["shadowsocks","vpn"],"title":"shadowsocks 服务端安装指南","uri":"/2022-03-09-shadowsocks/"},{"categories":null,"content":"Configuration 官方文档说配置文件在这里： Edit your config.json file. By default, it’s located in /usr/local/etc/shadowsocks-libev. 但我的不是。 编辑/etc/shadowsocks-libev/config.json { \"server\":\"0.0.0.0\", \"mode\":\"tcp_and_udp\", \"server_port\":your-port, \"password\":\"your-pass\", \"timeout\":300, \"method\":\"chacha20-ietf-poly1305\" } mode有三种：tcp_only，udp_only，tcp_and_udp。 “server”:“0.0.0.0” //只使用ipv4 “server”:[\"::0\",“0.0.0.0”] //使用ipv6和ipv4 server-port: 这个端口我之前用的443，因为公司网络只给了80和443出口。但是用443的时候很慢或者根本翻不出去，v2RayN日志如下： app/proxyman/outbound: failed to process outbound traffic \u003e proxy/shadowsocks: failed to find an available destination \u003e common/retry: [dial tcp x.x.x.x:443: i/o timeout dial tcp x.x.x.x:443: operation was canceled] \u003e common/retry: all retry attempts failed 换成别的端口如8888就可以了。80没试。 默认的 \"server\":[\"::1\", \"127.0.0.1\"]不行。 删掉了\"local_port\":1080，服务端不需要。 ","date":"2022-03-09","objectID":"/2022-03-09-shadowsocks/:2:0","tags":["shadowsocks","vpn"],"title":"shadowsocks 服务端安装指南","uri":"/2022-03-09-shadowsocks/"},{"categories":null,"content":"加速 Using TCP BBR1 echo net.core.default_qdisc=fq \u003e\u003e /etc/sysctl.conf echo net.ipv4.tcp_congestion_control=bbr \u003e\u003e /etc/sysctl.conf sysctl -p sysctl net.ipv4.tcp_available_congestion_control ","date":"2022-03-09","objectID":"/2022-03-09-shadowsocks/:3:0","tags":["shadowsocks","vpn"],"title":"shadowsocks 服务端安装指南","uri":"/2022-03-09-shadowsocks/"},{"categories":null,"content":"开机启动 sudo systemctl enable shadowsocks-libev.service # 禁用开机启动 sudo systemctl disable shadowsocks-libev.service 官方文档做法： To enable shadowsocks-libev, add the following rc variable to your /etc/rc.conf file: shadowsocks_libev_enable=\"YES\" ","date":"2022-03-09","objectID":"/2022-03-09-shadowsocks/:4:0","tags":["shadowsocks","vpn"],"title":"shadowsocks 服务端安装指南","uri":"/2022-03-09-shadowsocks/"},{"categories":null,"content":"Run Start the Shadowsocks server: systemctrl start shadowsocks-libev.service # 或者 service shadowsocks-libev start ","date":"2022-03-09","objectID":"/2022-03-09-shadowsocks/:5:0","tags":["shadowsocks","vpn"],"title":"shadowsocks 服务端安装指南","uri":"/2022-03-09-shadowsocks/"},{"categories":null,"content":"提示 shadowsocks的服务器在v2rayNG app上无法使用，需使用Shadowsocks app。但是PC上的v2rayN可以使用ss服务器。 ","date":"2022-03-09","objectID":"/2022-03-09-shadowsocks/:6:0","tags":["shadowsocks","vpn"],"title":"shadowsocks 服务端安装指南","uri":"/2022-03-09-shadowsocks/"},{"categories":null,"content":"Using TCP BBR TCP BBR is a TCP congestion control algorithm developed by Google and its been reported to improve performance on certain networks. You can enable it by adding the following to lines to your system configuration file. sudo nano /etc/sysctl.conf net.core.default_qdisc=fq net.ipv4.tcp_congestion_control=bbr Then save the file and reload the settings. sudo sysctl -p Check the changes by running the next command. sudo sysctl net.ipv4.tcp_congestion_control If the output is as follows the setting was applied successfully. net.ipv4.tcp_congestion_control = bbr These optimisations should help alleviate any possible performance issues. ↩︎ ","date":"2022-03-09","objectID":"/2022-03-09-shadowsocks/:6:1","tags":["shadowsocks","vpn"],"title":"shadowsocks 服务端安装指南","uri":"/2022-03-09-shadowsocks/"},{"categories":null,"content":" 所有的教程都不如官网。 ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:0:0","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"官网 官网：V2Fly.org github：v2fly/v2ray-core: A platform for building proxies to bypass network restrictions. (github.com) 以下是旧的官网和github： 官网：Project V · Project V 官方网站 (v2ray.com) github：v2ray/v2ray-core: A platform for building proxies to bypass network restrictions. (github.com) 注意，旧版本的安装脚本已废弃。 ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:1:0","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"安装 使用Linux安装脚本： v2fly/fhs-install-v2ray: Bash script for installing V2Ray in Linux that support systemd (github.com) 该脚本在运行时会提供 info 和 error 等信息，请仔细阅读。 ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:2:0","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"安装和更新 V2Ray // 安装可执行文件和 .dat 数据文件 # bash \u003c(curl -L https://raw.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-release.sh) ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:2:1","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"安装最新发行的 geoip.dat 和 geosite.dat // 只更新 .dat 数据文件 # bash \u003c(curl -L https://raw.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-dat-release.sh) ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:2:2","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"移除 V2Ray # bash \u003c(curl -L https://raw.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-release.sh) --remove 我的安装日志： # 先切换到root $ sudo su # 执行 root@ubuntu:~# bash \u003c(curl -L https://raw.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-release.sh) % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 21647 100 21647 0 0 1321k 0 --:--:-- --:--:-- --:--:-- 1321k info: Installing V2Ray v4.44.0 for aarch64 Downloading V2Ray archive: https://github.com/v2fly/v2ray-core/releases/download/v4.44.0/v2ray-linux-arm64-v8a.zip % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 664 100 664 0 0 2055 0 --:--:-- --:--:-- --:--:-- 2055 100 12.4M 100 12.4M 0 0 3138k 0 0:00:04 0:00:04 --:--:-- 4019k Downloading verification file for V2Ray archive: https://github.com/v2fly/v2ray-core/releases/download/v4.44.0/v2ray-linux-arm64-v8a.zip.dgst info: Extract the V2Ray package to /tmp/tmp.A891dl5AVC and prepare it for installation. info: Systemd service files have been installed successfully! warning: The following are the actual parameters for the v2ray service startup. warning: Please make sure the configuration file path is correctly set. # /etc/systemd/system/v2ray.service [Unit] Description=V2Ray Service Documentation=https://www.v2fly.org/ After=network.target nss-lookup.target [Service] User=nobody CapabilityBoundingSet=CAP_NET_ADMIN CAP_NET_BIND_SERVICE AmbientCapabilities=CAP_NET_ADMIN CAP_NET_BIND_SERVICE NoNewPrivileges=true ExecStart=/usr/local/bin/v2ray -config /usr/local/etc/v2ray/config.json Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target # /etc/systemd/system/v2ray.service.d/10-donot_touch_single_conf.conf # In case you have a good reason to do so, duplicate this file in the same directory and make your customizes there. # Or all changes you made will be lost! # Refer: https://www.freedesktop.org/software/systemd/man/systemd.unit.html [Service] ExecStart= ExecStart=/usr/local/bin/v2ray -config /usr/local/etc/v2ray/config.json installed: /usr/local/bin/v2ray installed: /usr/local/bin/v2ctl installed: /usr/local/share/v2ray/geoip.dat installed: /usr/local/share/v2ray/geosite.dat installed: /usr/local/etc/v2ray/config.json installed: /var/log/v2ray/ installed: /var/log/v2ray/access.log installed: /var/log/v2ray/error.log installed: /etc/systemd/system/v2ray.service installed: /etc/systemd/system/v2ray@.service removed: /tmp/tmp.A891dl5AVC info: V2Ray v4.44.0 is installed. You may need to execute a command to remove dependent software: apt purge curl unzip Please execute the command: systemctl enable v2ray; systemctl start v2ray root@instance-20220307-2119:/home/ubuntu/v2ray# systemctl status v2ray ● v2ray.service - V2Ray Service Loaded: loaded (/etc/systemd/system/v2ray.service; disabled; vendor preset: enabled) Drop-In: /etc/systemd/system/v2ray.service.d └─10-donot_touch_single_conf.conf Active: inactive (dead) Docs: https://www.v2fly.org/ ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:2:3","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"配置 官方给了模板：v2fly/v2ray-examples: v2ray-core 的模板们 (github.com) 如果想了解更多，还有一份详细文档：V2Ray 配置指南 | 新 V2Ray 白话文指南 (v2fly.org) ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:3:0","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"一份简单的配置 来源：VMess | 新 V2Ray 白话文指南 (v2fly.org) 服务端：配置文件位于/usr/local/etc/v2ray/config.json { \"inbounds\": [ { \"port\": 16823, // 服务器监听端口 \"protocol\": \"vmess\", // 主传入协议 \"settings\": { \"clients\": [ { \"id\": \"b831381d-6324-4d53-ad4f-8cda48b30811\", // 用户 ID，客户端与服务器必须相同 \"alterId\": 0 // 新版本不能设置为64 } ] } } ], \"outbounds\": [ { \"protocol\": \"freedom\", // 主传出协议 \"settings\": {} } ] } 客户端 { \"inbounds\": [ { \"port\": 1080, // 监听端口 \"protocol\": \"socks\", // 入口协议为 SOCKS 5 \"sniffing\": { \"enabled\": true, \"destOverride\": [\"http\", \"tls\"] }, \"settings\": { \"auth\": \"noauth\" //socks的认证设置，noauth 代表不认证，由于 socks 通常在客户端使用，所以这里不认证 } } ], \"outbounds\": [ { \"protocol\": \"vmess\", // 出口协议 \"settings\": { \"vnext\": [ { \"address\": \"serveraddr.com\", // 服务器地址，请修改为你自己的服务器 IP 或域名 \"port\": 16823, // 服务器端口 \"users\": [ { \"id\": \"b831381d-6324-4d53-ad4f-8cda48b30811\", // 用户 ID，必须与服务器端配置相同 \"alterId\": 0 // 此处的值也应当与服务器相同 } ] } ] } } ] } ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:3:1","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"如何生成 UUID 在线生成器：Online UUID Generator Tool Linux 命令生成：cat /proc/sys/kernel/random/uuid 版本 对于“变体（variants）1”和“变体2”，标准中定义了五个版本（versions），并且在特定用例中每个版本可能比其他版本更合适。 版本由 M 字符串中指示。 版本1 - UUID 是根据时间和 节点ID（通常是MAC地址）生成； 版本2 - UUID是根据标识符（通常是组或用户ID）、时间和节点ID生成； 版本3、版本5 - 确定性UUID 通过散列（hashing）名字空间（namespace）标识符和名称生成； 版本4 - UUID 使用随机性或伪随机性生成。 —— 来源：通用唯一识别码 - 维基百科，自由的百科全书 (wikipedia.org) ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:3:2","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"如何选择配置方案 官方的这个图真的太皮了，特意拿过来： ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:3:3","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":null,"content":"运行 sudo systemctl start v2ray 开机自启： sudo systemctl enable v2ray.service Created symlink /etc/systemd/system/multi-user.target.wants/v2ray.service → /etc/systemd/system/v2ray.service. ","date":"2022-03-08","objectID":"/2022-03-08-v2ray/:4:0","tags":["v2ray","vpn"],"title":"V2Ray 搭建指南","uri":"/2022-03-08-v2ray/"},{"categories":["springboot"],"content":"SpringBoot依赖管理的能力 SpringBoot管理的依赖有1000多个，详见Dependency Versions (spring.io)。这些组件，在和SpringBoot一起使用的时候，可以不指定版本，因为SpringBoot预定义了合适的版本。这样做的好处是大大降低了依赖冲突的概率。 例如，对于这样一个项目（使用了web和kafka-clients）： plugins { id 'org.springframework.boot' version '2.6.4' id 'io.spring.dependency-management' version '1.0.11.RELEASE' id 'java' } group = 'cn.whu.wy' version = '0.0.1-SNAPSHOT' sourceCompatibility = '11' repositories { maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation group: 'org.apache.kafka', name: 'kafka-clients', version: '2.6.0' testImplementation 'org.springframework.boot:spring-boot-starter-test' } tasks.named('test') { useJUnitPlatform() } 指定了kafka-clients使用2.6.0版本，依赖面板显示： 实际上，由于kafka-clients也纳入了SpringBoot的依赖管理，可以不指定版本号。去掉version: '2.6.0'之后，依赖面板显示： 可见，SpringBoot 2.6.4预定义的Kafka版本为3.0。 ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:1:0","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":["springboot"],"content":"SpringBoot的依赖 在Spring Initializr上创建一个简单的SpringBoot项目（web+jdbc），生成的依赖为： plugins { id 'org.springframework.boot' version '2.7.0' id 'io.spring.dependency-management' version '1.0.11.RELEASE' id 'java' } group = 'com.example' version = '0.0.1-SNAPSHOT' sourceCompatibility = '11' configurations { compileOnly { extendsFrom annotationProcessor } } repositories { mavenCentral() } dependencies { implementation 'org.springframework.boot:spring-boot-starter-jdbc' implementation 'org.springframework.boot:spring-boot-starter-web' compileOnly 'org.projectlombok:lombok' runtimeOnly 'mysql:mysql-connector-java' annotationProcessor 'org.projectlombok:lombok' testImplementation 'org.springframework.boot:spring-boot-starter-test' } tasks.named('test') { useJUnitPlatform() } ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:2:0","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":["springboot"],"content":"SpringCloud的依赖 如果还勾选了SpringCloud的组件（如openfeign），生成的依赖为： plugins { id 'org.springframework.boot' version '2.7.0' id 'io.spring.dependency-management' version '1.0.11.RELEASE' id 'java' } group = 'com.example' version = '0.0.1-SNAPSHOT' sourceCompatibility = '11' configurations { compileOnly { extendsFrom annotationProcessor } } repositories { mavenCentral() } ext { set('springCloudVersion', \"2021.0.3\") } dependencies { implementation 'org.springframework.boot:spring-boot-starter-jdbc' implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.springframework.cloud:spring-cloud-starter-openfeign' compileOnly 'org.projectlombok:lombok' runtimeOnly 'mysql:mysql-connector-java' annotationProcessor 'org.projectlombok:lombok' testImplementation 'org.springframework.boot:spring-boot-starter-test' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } tasks.named('test') { useJUnitPlatform() } 可见，比起只使用SpringBoot的组件，多了以下内容： ext { set('springCloudVersion', \"2021.0.3\") } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:3:0","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":["springboot"],"content":"org.springframework.boot与io.spring.dependency-management的区别 上面的几个示例都使用了两个插件：org.springframework.boot（以下简称springboot插件）和io.spring.dependency-management（以下简称dependency-management插件）。这俩有什么区别？ springboot插件表示这是一个SpringBoot项目，可以build为一个可执行的jar包（使用下图build面板下的bootJar命令），项目里面必须有一个使用@SpringBootApplication标记的类，该类是可执行jar的入口类。 dependency-management插件，从名字就能看出来是管理依赖的。如下图，help下面的dependencyXXX等命令就是该插件提供的： 双击运行dependencyManagement命令，会打印出Default dependency management for all configurations： 这些配置就是你正在用的SpringBoot版本所管理的依赖，与docs.spring.io上提供的是一致的。比如我正在使用SpringBoot 2.6.4，dependencyManagement命令打印的内容与https://docs.spring.io/spring-boot/docs/2.6.4/reference/html/dependency-versions.html相同。 ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:4:0","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":["springboot"],"content":"两个插件的联系 When you apply the io.spring.dependency-management plugin, Spring Boot’s plugin will automatically import the spring-boot-dependencies bom from the version of Spring Boot that you are using. ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:4:1","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":["springboot"],"content":"非SpringBoot项目如何使用SpringBoot管理依赖 Ref：Using Spring Boot’s Dependency Management in Isolation 有些项目并不是SpringBoot项目（不需要build为可执行的jar包），又想使用SpringBoot的依赖管理功能，该怎么做？ plugins { // configure the project to depend on the Spring Boot plugin but do not apply it: id 'org.springframework.boot' version '2.6.4' apply false // look here! id 'java' } // add this apply plugin: 'io.spring.dependency-management' dependencyManagement { imports { mavenBom(org.springframework.boot.gradle.plugin.SpringBootPlugin.BOM_COORDINATES) } } ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:5:0","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":["springboot"],"content":"gradle legacy plugin application（老式写法） 以下是一个具备依赖管理功能的可运行的SpringBoot项目： buildscript { repositories { maven { url \"https://plugins.gradle.org/m2/\" } } dependencies { classpath \"org.springframework.boot:spring-boot-gradle-plugin:2.7.0\" classpath \"io.spring.gradle:dependency-management-plugin:1.0.11.RELEASE\" } } apply plugin: \"org.springframework.boot\" apply plugin: \"io.spring.dependency-management\" // 下面的group、version、sourceCompatibility、repositories{}、dependencyManagement{}、dependencies{} 等是一样的，略 以下是一个具备依赖管理功能的不可运行的SpringBoot项目： buildscript { repositories { maven { url \"https://plugins.gradle.org/m2/\" } } dependencies { classpath \"org.springframework.boot:spring-boot-gradle-plugin:2.7.0\" } } apply plugin: \"io.spring.dependency-management\" dependencyManagement { imports { mavenBom(org.springframework.boot.gradle.plugin.SpringBootPlugin.BOM_COORDINATES) } } // 下面的group、version、sourceCompatibility、repositories{}、dependencyManagement{}、dependencies{} 等是一样的，略 ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:6:0","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":["springboot"],"content":"maven spring-boot-starter-parent 依赖 spring-boot-starter-parent 方便快捷 自动引入 spring-boot-dependencies 自动配置 spring-boot-maven-plugin \u003cparent\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-parent\u003c/artifactId\u003e \u003cversion\u003e2.1.1.RELEASE\u003c/version\u003e \u003crelativePath/\u003e \u003c!-- lookup parent from repository --\u003e \u003c/parent\u003e 不依赖 spring-boot-starter-parent \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-dependencies\u003c/artifactId\u003e \u003cversion\u003e2.1.1.RELEASE\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cscope\u003eimport\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e2.1.1.RELEASE\u003c/version\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cgoals\u003e \u003cgoal\u003erepackage\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:7:0","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":["springboot"],"content":"Reference Dependency Versions (spring.io) Dependency Management Plugin (spring.io) Spring Boot Gradle Plugin Reference Guide ","date":"2020-11-14","objectID":"/2020-11-14-spring-dependency-management/:8:0","tags":["springboot","springcloud","spring依赖管理"],"title":"Spring的依赖管理","uri":"/2020-11-14-spring-dependency-management/"},{"categories":null,"content":"关于本站 ","date":"0001-01-01","objectID":"/about/:1:0","tags":null,"title":"Hi，欢迎登船","uri":"/about/"},{"categories":null,"content":"缘起 呃，这是我的第几个博客了…… 第1个是中学时期的QQ空间。 第2个是刚上大学开始写的新浪博客，2010年前后，新浪博客如火如荼。这个博客当做日记本在用，记录了很多和同学、朋友、女朋友的事情。 第3个也是新浪博客。由于上一个新浪博客是个情感博客，不太好拿出去和别人交流，因此新开一个技术博客。这个博客写了很长时间，一直到2015年。共有299篇，18万访问，博客等级17级。 第4个博客是博客园(传送门)，第一篇是写于2013年3月5日的如何实现手机和PC用NFC通信，一晃9年过去了。至今有184篇，32万访问。同时期还开了一个CSDN的博客，比较后更喜欢博客园的调性，因此CSDN上只写过一篇。 从2018年开始，博客更新很慢，一年三五篇。因为都记录在有道云笔记了，写博客是要发布的，没有写笔记那么随意。 第5-7个博客都是搭在github pages上的，此站的前身。分别是用Hexo、Hugo、Jekyll构建的，都已经铲掉了。但是文章都在保留在笔记软件内。铁打的笔记，流水的博客。现在（2022-04-03）又回到了Hugo，决定不再折腾，认真写内容。 这次重新开博，导火索是博客园被整顿了，根本原因是写的笔记只能自己看（甚至自己都不看，写了就写了），不能像博客那样与人交流，共同进步。比起写笔记，写博客更正式，更能锻炼写作能力——四年不怎么写东西了，发现现在话都说不明白了…… ","date":"0001-01-01","objectID":"/about/:1:1","tags":null,"title":"Hi，欢迎登船","uri":"/about/"},{"categories":null,"content":"部署 本站在三个地方都有部署，您可以选择一个访问速度最快的进行使用。 github.io netlify.app vercel.app ","date":"0001-01-01","objectID":"/about/:1:2","tags":null,"title":"Hi，欢迎登船","uri":"/about/"},{"categories":null,"content":"关于我 重庆18年 + 武汉7年 + 深圳5+年 = 30+岁 Java + Python + JavaScript + Linux + MySQL + Kafka + Zookeeper + SpringBoot + … = 啥也不会 Bilibili + 公众号 + 小视频 + 知乎 + 抖音 + 相机 = 想搞点内容创作 基金 + 中概互联 + 2022 = 老乡别走 \u0026 关灯吃面 发福 + 弱鸡 = 早睡早起 \u0026 少吃多动 ","date":"0001-01-01","objectID":"/about/:2:0","tags":null,"title":"Hi，欢迎登船","uri":"/about/"},{"categories":null,"content":"特别 好基友的菜园子 ","date":"0001-01-01","objectID":"/links/:1:0","tags":null,"title":"友情链接","uri":"/links/"},{"categories":null,"content":"你看看人家 工程师，而立已婚有娃，宅叔一枚，着迷于所有看起来很酷很性感的东西 欧阳荣 jdhao’s digital space ","date":"0001-01-01","objectID":"/links/:2:0","tags":null,"title":"友情链接","uri":"/links/"}]